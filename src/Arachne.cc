/* Copyright (c) 2015-2018 Stanford University
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR(S) DISCLAIM ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL AUTHORS BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <stdio.h>
#include <thread>
#include "CoreArbiter/CoreArbiterClient.h"
#include "CorePolicy.h"
#include "DefaultCorePolicy.h"
#include "PerfUtils/TimeTrace.h"
#include "PerfUtils/Util.h"

#include "Arachne.h"
#include "CoreArbiter/ArbiterClientShim.h"

namespace Arachne {

// Change 0 -> 1 in the following line to compile detailed time tracing in
// this file.
#define TIME_TRACE 0

using CoreArbiter::CoreArbiterClient;
using PerfUtils::Cycles;
using PerfUtils::TimeTrace;
using PerfUtils::Util::prefetch;

/**
 * This variable prevents multiple initializations of the library, but does
 * not protect against the user calling Arachne functions without initializing
 * the library, which results in undefined behavior.
 */
bool initialized = false;

// Used to allow redirection of error messages generated by the library.
FILE* errorStream = stderr;

// This function is provided by applications to perform initialization of any
// state local to each kernel thread. Initialization will happen once in the
// lifetime of each kernel thread.
std::function<void()> initCore = nullptr;

// The following configuration options can be passed into init.

/**
 * The minimum number of cores this application needs to run. This number must
 * be at least one higher than the number of cores which are permanently
 * exclusive.
 *
 * If this is set higher than the number of physical cores, the kernel will
 * multiplex, which is usually undesirable except when running unit tests on a
 * single-core system.
 */
volatile uint32_t minNumCores;

/**
 * This tracks the number of cores Arachne currently controls.
 */
std::atomic<uint32_t> numActiveCores;

/**
 * The largest number of cores that Arachne is permitted to utilize.  It is an
 * invariant that maxNumCores >= numActiveCores, but if the user explicitly
 * sets both then numActiveCores will push maxNumCores up to satisfy the
 * invariant.
 */
volatile uint32_t maxNumCores;

/**
 * Used to ensure that only one thread attempts to become exclusive or shared
 * at a time. This protects against a thread being migrated while it is
 * halfway through makeExclusiveOnCore and making incorrect assumptions about
 * the core it is currently on.
 */
SpinLock coreExclusionMutex;

/**
 * Configurable maximum stack size for all threads.
 */
int stackSize = 1024 * 1024;

/**
 * Keep track of the kernel threads we are running so that we can join them on
 * destruction. Also, store a pointer to the original stacks to facilitate
 * switching back and joining.
 */
std::vector<std::thread> kernelThreads;
std::vector<void*> kernelThreadStacks;

/**
 * Alert the kernel threads that they should exit immediately. This is used
 * only for testing.
 */
volatile bool shutdown;

/**
 * Constrol the maximum number of times we will try to migrate a context when
 * performing migration before admitting defeat.
 */
const int MAX_MIGRATION_RETRIES = 10;

/**
 * The collection of possibly runnable contexts for each kernel thread.
 */
std::vector<ThreadContext**> allThreadContexts;

/**
 * Each element points at the occupiedAndCount belonging to the core with the
 * coreId equal to its index.
 */
std::vector<std::atomic<MaskAndCount>*> occupiedAndCount;

/**
 * This is a per-core bitmask that represents which contexts are pinned to the
 * core (such contexts cannot be migrated away from the core).
 */
std::vector<std::atomic<uint64_t>*> pinnedContexts;

/**
 * Track the addresses of each kernel thread's
 * Dispatch::lastTotalCollectionTime.
 */
std::vector<uint64_t*> lastTotalCollectionTime;

/**
 * Setting a jth bit in the ith element of this vector indicates that the
 * priority of the thread living at index j on core i is temporarily raised.
 * Values pointed to must be in separate cache lines for high performance.
 */
std::vector<std::atomic<uint64_t>*> allHighPriorityThreads;

/**
 * An array of semaphores cores can park on to idle themselves.
 * Indices correspond to individual core IDs.
 */
std::vector<::Semaphore*> coreIdleSemaphores;

/**
 * All core-specific state that is not associated with other classes.
 */
thread_local Core core;

// BEGIN Testing-Specific Flags
bool disableLoadEstimation;
// END   Testing-Specific Flags

// Allocate storage for load-tracking pointers.
thread_local uint64_t IdleTimeTracker::lastTotalCollectionTime;
thread_local uint64_t IdleTimeTracker::dispatchStartCycles;
thread_local uint64_t IdleTimeTracker::lastDispatchIterationStart;
thread_local uint8_t IdleTimeTracker::numThreadsRan;

// Allocate storage for nested dispatch detection.
thread_local bool NestedDispatchDetector::dispatchRunning;

/**
 * This variable defines whether Arachne should use the core arbiter.
 * It is (will be) parsed in from the command line.
 */
bool useCoreArbiter = true;

std::string coreArbiterSocketPath;
CoreArbiterClient* coreArbiter = NULL;

/*
 *  The CorePolicy that Arachne will use.
 */
CorePolicy* corePolicy = NULL;

// Forward declarations
void releaseCore();
void descheduleCore();
void idleCorePrivate();
void checkForArbiterRequest();

// The following constants must be defined here because we want them to be
// scoped inside their respective structures, but gtest macros try to take
// their address, and supplying their values inside the declaration causes an
// undefined reference error.
const uint64_t ThreadContext::BLOCKED = ~0L;
const uint64_t ThreadContext::UNOCCUPIED = ~0L - 1;
const uint8_t ThreadContext::CORE_UNASSIGNED = ~0L;
const uint8_t MaskAndCount::EXCLUSIVE = maxThreadsPerCore * 2 + 1;

/**
 * Allocate a block of memory aligned at the beginning of a cache line.
 *
 * \param size
 *     The amount of memory to allocate.
 * \param alignment
 *     The address of the allocated memory will be a multiple of this
 *     parameter, which must be a power of two and a multiple of sizeof(void *)
 */
void*
alignedAlloc(size_t size, size_t alignment) {
    void* temp;
    int result = posix_memalign(&temp, alignment, size);
    if (result != 0) {
        ARACHNE_LOG(ERROR, "posix_memalign returned %s", strerror(result));
        abort();
    }
    assert((reinterpret_cast<uint64_t>(temp) & (alignment - 1)) == 0);
    return temp;
}

/**
 * Initialize thread local data structures that will later be "registered"
 * in a global array depending on the real core id assigned by the core
 * arbiter.
 */
void
initializeCore(Core* core) {
    core->localPinnedContexts = reinterpret_cast<std::atomic<uint64_t>*>(
        alignedAlloc(sizeof(uint64_t)));
    // All cores initially poll for work on context 0's stack.
    core->localPinnedContexts->store(1U);

    core->highPriorityThreads = reinterpret_cast<std::atomic<uint64_t>*>(
        alignedAlloc(sizeof(std::atomic<uint64_t>)));
    memset(core->highPriorityThreads, 0, sizeof(std::atomic<uint64_t>));

    // Allocate stacks and contexts
    ThreadContext** contexts = new ThreadContext*[maxThreadsPerCore];
    for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
        contexts[k] = reinterpret_cast<ThreadContext*>(
            alignedAlloc(sizeof(ThreadContext)));
        new (contexts[k]) ThreadContext(k);
    }
    core->localThreadContexts = contexts;
}

/**
 * Release resources allocated during initializeCore().
 */
void
deinitializeCore(Core* core) {
    free(core->localPinnedContexts);
    free(core->highPriorityThreads);
}

/**
 * Main function for a kernel thread, which roughly corresponds to a core.
 */
void
threadMain() {
    if (initCore)
        initCore();

    initializeCore(&core);
    for (;;) {
        // Get id from coreArbiter
        core.id = coreArbiter->blockUntilCoreAvailable();

        // Associate this thread's PerfStats pointer with the proper value for
        // the core this thread is executing on.
        PerfStats::threadStats = PerfStats::getStats(core.id);

        // Register the address of our IdleTimeTracker::lastTotalCollectionTime
        // with a global index.
        lastTotalCollectionTime[core.id] =
            &IdleTimeTracker::lastTotalCollectionTime;
        // Prevent the use of abandoned ThreadContext which occurred as a
        // result of a shutdown request.
        if (shutdown) {
            PerfStats::releaseStats(std::move(PerfStats::threadStats));
            break;
        }
        core.localOccupiedAndCount = occupiedAndCount[core.id];
        pinnedContexts[core.id] = core.localPinnedContexts;
        core.localThreadContexts = allThreadContexts[core.id];
        allHighPriorityThreads[core.id] = core.highPriorityThreads;

        IdleTimeTracker::lastTotalCollectionTime = 0;
        // Clean up state from the last time this thread ran. This should
        // eventually be removed once we ensure that cleanup happens on
        // descheduling.
        *core.localOccupiedAndCount = {0, 0};
        *core.highPriorityThreads = 0;
        core.privatePriorityMask = 0;
        core.coreDeschedulingScheduled = false;

        // Correct the ThreadContext.coreId() here to match the current core.
        // We must do these operations before making cores available for
        // scheduling because otherwise our ThreadContexts may be targeted for
        // migration before their stacks are initialized.
        for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
            core.localThreadContexts[k]->coreId = static_cast<uint8_t>(core.id);
            core.localThreadContexts[k]->originalCoreId =
                static_cast<uint8_t>(core.id);
            core.localThreadContexts[k]->initializeStack();
        }

        // This marks the point at which new thread creations may begin.
        corePolicy->coreAvailable(core.id);
        numActiveCores++;
        ARACHNE_LOG(DEBUG, "Number of cores increased from %d to %d\n",
                    numActiveCores - 1, numActiveCores.load());
#if TIME_TRACE
        TimeTrace::record("Core Count %d --> %d", numActiveCores - 1,
                          numActiveCores.load());
#endif
        PerfStats::threadStats->numCoreIncrements++;

        // Correct statistics
        IdleTimeTracker::numThreadsRan = 0;
        IdleTimeTracker::lastDispatchIterationStart = Cycles::rdtsc();

        // Poll for work to do using the first context
        core.loadedContext = core.localThreadContexts[0];

        // Transfers control to the Arachne dispatcher.
        // This context has been pre-initialized by init so it will "return"
        // to the schedulerMainLoop.
        swapcontext(&core.loadedContext->sp, &kernelThreadStacks[core.id]);
        numActiveCores--;
        if (shutdown) {
            // Avoid leaking PerfStats across shutdowns.
            PerfStats::releaseStats(std::move(PerfStats::threadStats));
            break;
        }
        ARACHNE_LOG(DEBUG,
                    "Number of cores decreased from %d to %d\n; Core %d "
                    "going offline.",
                    numActiveCores + 1, numActiveCores.load(), core.id);
#if TIME_TRACE
        TimeTrace::record("Core Count %d --> %d", numActiveCores + 1,
                          numActiveCores.load());
#endif
        PerfStats::threadStats->numCoreDecrements++;

        // Release the PerfStats associated with this core, since we are about
        // to give up this core.
        PerfStats::releaseStats(std::move(PerfStats::threadStats));
    }
    deinitializeCore(&core);

    coreArbiter->unregisterThread();
}

/**
 * Save the current register values onto one stack and load fresh register
 * values from another stack.
 * This method does not return to its caller immediately. It returns to the
 * caller when another thread on the same kernel thread invokes this method
 * with the current value of target as the saved parameter.
 *
 * \param saved
 *     Address of the stack location to load register values from.
 * \param target
 *     Address of the stack location to save register values to.
 */
void __attribute__((noinline)) swapcontext(void** saved, void** target) {
    // This code depends on knowledge of the compiler's calling convention: rdi
    // and rsi are the first two arguments.
    // Alternative approaches tend to run into conflicts with compiler register
    // use.

    // Save the registers that the compiler expects to persist across method
    // calls and store the stack pointer's location after saving these
    // registers.
    // NB: The space used by the pushed and
    // popped registers must equal the value of SPACE_FOR_SAVED_REGISTERS, which
    // should be updated atomically with this assembly.
    asm("pushq %r12\n\t"
        "pushq %r13\n\t"
        "pushq %r14\n\t"
        "pushq %r15\n\t"
        "pushq %rbx\n\t"
        "pushq %rbp\n\t"
        "movq %rsp, (%rsi)");

    // Load the stack pointer and restore the registers
    asm("movq (%rdi), %rsp\n\t"
        "popq %rbp\n\t"
        "popq %rbx\n\t"
        "popq %r15\n\t"
        "popq %r14\n\t"
        "popq %r13\n\t"
        "popq %r12");
}

/**
 * This is the top level method executed by each thread context. It is never
 * directly invoked. Instead, the thread's context is set up to "return" to
 * this method when we context switch to it the first time.
 */
void
schedulerMainLoop() {
    // Swapping to a brand new context for the first time after some other
    // context has entered dispatch() will cause the NestedDispatchDetector to
    // fire. This check prevents it from firing, but anyone seeking to modify
    // dispatch() should ensure that it still behaves correctly when dispatch
    // is invoked from the top in new contexts while old contexts are swapped
    // out in the middle of dispatch().
    NestedDispatchDetector::clearDispatchFlag();
    while (true) {
        // Check for whether this thread should exit, for the purposes of
        // ramping down.
        if (core.coreReadyForReturnToArbiter) {
            // Switch back to our kernel-provided stack to block in the Core
            // Arbiter, since the next time this thread unblocks, it may not
            // live on the same core, and will use a different set of user
            // contexts.
            core.coreReadyForReturnToArbiter = false;
            swapcontext(&kernelThreadStacks[core.id], &core.loadedContext->sp);
        }
        // No thread to execute yet. This call will not return until we have
        // been assigned a new Arachne thread.
        dispatch();
        reinterpret_cast<ThreadInvocationEnabler*>(
            &core.loadedContext->threadInvocation)
            ->runThread();
        // The thread has exited.
        // Cancel any wakeups the thread may have scheduled for itself before
        // exiting.
        core.loadedContext->wakeupTimeInCycles = ThreadContext::UNOCCUPIED;

        prefetch(core.localOccupiedAndCount);
        // The positioning of this lock is rather subtle, and makes the
        // following three operations atomic.
        //   1. Bumping the generation number.
        //   2. Clearing the occupied bit for this context.
        //   3. Notifying joiners that this thread has fully exited.

        // It is important to notify joiners only after we have cleared our
        // occupied bit, because thread creations by the joiner will fail
        // even if this thread has logically exited. However, this context
        // cannot contend for a lock after its occupied bit has been cleared,
        // because it would never awaken once it started to spin. Thus, the
        // lock must be taken and held throughout the process of clearing the
        // occupied bit and notifying threads attempting to join this thread.
        std::lock_guard<SpinLock> joinGuard(core.loadedContext->joinLock);

        // Bump the generation number for the next newborn thread. This must be
        // done under the joinLock, since any joiner that observed the new
        // generation number might assume that the occupied bit for this
        // context is already cleared.
        core.loadedContext->generation++;

        // Pin the current context before clearing the occupied bit.
        uint64_t pinMask = 1L << core.loadedContext->idInCore;
        core.localPinnedContexts->store(pinMask, std::memory_order_release);

        // The code below clears the occupied flag for the current
        // ThreadContext.
        //
        // While this logically comes before dispatch(), it is here to prevent
        // it from racing against thread creations that come before the start
        // of the outer loop, since the occupied flags for such creations would
        // get wiped out by this code.
        bool success;
        MaskAndCount slotMap;
        do {
            slotMap = *core.localOccupiedAndCount;
            MaskAndCount oldSlotMap = slotMap;
            if (slotMap.numOccupied == 0) {
                ARACHNE_LOG(ERROR,
                            "Thread Exit on Core %d: Detected numOccupied == 0 "
                            "occupied %lu\n",
                            core.id, slotMap.occupied);
                abort();
            }
            slotMap.numOccupied--;

            slotMap.occupied = slotMap.occupied &
                               ~(1L << core.loadedContext->idInCore) &
                               0x00FFFFFFFFFFFFFF;
            success = core.localOccupiedAndCount->compare_exchange_strong(
                oldSlotMap, slotMap, std::memory_order_acq_rel);
        } while (!success);

        // Reset highestOccupiedContext based on value of occupied flag, which
        // we just CASed in, and the pinned context mask, which we just set.
        // It is necessary to consider pinMask because it forces other cores
        // attempting to migrate threads to this core to target contexts at
        // slots above the pinned context, even if the pinned context is
        // unocupied, which creates a gap between highestOccupiedContext and
        // the next occupied context.
        uint64_t occupiedOrPinned = slotMap.occupied | pinMask;

        // The result of __builtin_clzll is undefined if occupied is 0.
        // The function __builtin_clzll returns the number leading 0-bits,
        // so subtract the return value of 63 to get a zero-based bit index
        core.highestOccupiedContext =
            occupiedOrPinned == 0
                ? 0
                : static_cast<uint8_t>(63 - __builtin_clzll(occupiedOrPinned));

        // Newborn threads should not have elevated priority, even if the
        // predecessors had leftover priority
        core.privatePriorityMask &= ~(1L << (core.loadedContext->idInCore));
        *core.highPriorityThreads &= ~(1L << (core.loadedContext->idInCore));
        PerfStats::threadStats->numThreadsFinished++;

        core.loadedContext->joinCV.notifyAll();
    }
}

/**
 * This method is used as part of cooperative multithreading to give other
 * Arachne threads on the same core a chance to run.
 * It will return when all other threads have had a chance to run.
 */
void
yield() {
    // This check makes yield() callable from a non-Arachne thread without
    // error.
    if (!core.loadedContext)
        return;
    if (core.localOccupiedAndCount->load().numOccupied == 1 && !shutdown &&
        !core.coreReadyForReturnToArbiter) {
        // Even if the current core is running a single Arachne thread, it must
        // still check for the preemption by the core arbiter. This check is
        // typically done in dispatch(), but we skip going through the dispatch
        // loop as an optimization.
        checkForArbiterRequest();
        return;
    }
    // This thread is still runnable since it is merely yielding.
    core.loadedContext->wakeupTimeInCycles = 0L;
    dispatch();
}

/**
 * Sleep for at least ns nanoseconds. The amount of additional delay may be
 * impacted by other threads' activities such as blocking and yielding.
 */
void
sleep(uint64_t ns) {
    sleepForCycles(Cycles::fromNanoseconds(ns));
}

/**
 * Sleep for at least cycles cycles. The amount of additional delay may be
 * impacted by other threads' activities such as blocking and yielding.
 */
void
sleepForCycles(uint64_t cycles) {
    core.loadedContext->wakeupTimeInCycles = Cycles::rdtsc() + cycles;
    dispatch();
}

/**
 * Return a thread handle for the currently executing thread, identical to the
 * one returned by the createThread call that initially created this thread.
 *
 * When invoked from a non-Arachne thread, this function returns
 * Arachne::NullThread.
 */
ThreadId
getThreadId() {
    // This check enables the caller to determine whether or not they are
    // running in an Arachne thread.
    return core.loadedContext
               ? ThreadId(core.loadedContext, core.loadedContext->generation)
               : Arachne::NullThread;
}

/**
 * Deschedule the current thread until its wakeup time is reached (which may
 * have already happened) and find another thread to run. All direct and
 * indirect callers of this function must ensure that spurious wakeups are
 * safe.
 */
void
dispatch() {
    NestedDispatchDetector detector;
    IdleTimeTracker idleTimeTracker;
    Core& core = Arachne::core;
    // Cache the original context so that we can survive across migrations to
    // other kernel threads, since core.loadedContext is not reloaded correctly
    // from TLS after switching back to this context.
    ThreadContext* originalContext = core.loadedContext;
    if (unlikely(*reinterpret_cast<uint64_t*>(core.loadedContext->stack) !=
                 STACK_CANARY)) {
        ARACHNE_LOG(ERROR,
                    "Stack overflow detected on %p. Canary = %lu."
                    " Aborting...\n",
                    core.loadedContext,
                    *reinterpret_cast<uint64_t*>(core.loadedContext->stack));
        abort();
    }

    // Check for core release request once before checking for high priority
    // threads.
    checkForArbiterRequest();

    uint64_t dispatchIterationStartCycles = Cycles::rdtsc();

    // Check for high priority threads.
    if (!core.privatePriorityMask) {
        // Snapshot the high-priority threads in a core-local data structure
        // and process all of them before the next snapshot; this avoids cache
        // contention every time the priority of a thread is raised, and
        // ensures that one high priority thread cannot starve out another.
        core.privatePriorityMask = *core.highPriorityThreads;
        if (core.privatePriorityMask)
            *core.highPriorityThreads &= ~core.privatePriorityMask;
    }

    // Run any high priority threads before searching the entire set of
    // contexts for runnable threads.
    if (core.privatePriorityMask) {
        // This position is one-indexed with zero meaning that no bits were
        // set.
        int firstSetBit = ffsll(core.privatePriorityMask) - 1;

        core.privatePriorityMask &= ~(1L << (firstSetBit));

        ThreadContext* targetContext = core.localThreadContexts[firstSetBit];

        // Verify wakeup and occupied.
        if (targetContext->wakeupTimeInCycles == 0) {
            if (targetContext == core.loadedContext) {
                core.loadedContext->wakeupTimeInCycles = ThreadContext::BLOCKED;
                IdleTimeTracker::numThreadsRan++;

                // It is necessary to update core.highestOccupiedContext
                // here because two simultaneous returns from these
                // priority checks (the higher index blocking and the lower
                // index exiting) can result in a gap that cannot be
                // bridged by the core.highestOccupiedContext increment
                // mechanism below.
                core.highestOccupiedContext = std::max(
                    core.highestOccupiedContext, core.loadedContext->idInCore);
                return;
            }
            void** saved = &core.loadedContext->sp;
            core.loadedContext = targetContext;

            // Flush the idle cycle counter before a context switch because
            // switching to a fresh (previously unused) context will cause
            // dispatch to be called from the top again before this
            // invocation returns. This is problematic because it resets
            // dispatchStartCycles (used for computing idle cycles) but not
            // lastTotalCollectionTime (used for computing total cycles).
            idleTimeTracker.updatePerfStats();
            swapcontext(&core.loadedContext->sp, saved);
            originalContext->wakeupTimeInCycles = ThreadContext::BLOCKED;
            IdleTimeTracker::numThreadsRan++;
            Arachne::core.highestOccupiedContext = std::max(
                core.highestOccupiedContext, core.loadedContext->idInCore);
            return;
        }
    }
    // Find a thread to switch to
    uint8_t currentIndex = core.nextCandidateIndex;

    for (;; currentIndex++) {
        // Ensure that we do not go out of bounds.
        if (currentIndex == maxThreadsPerCore) {
            currentIndex = 0;
        }

        // At this point, it is guaranteed that it is safe to read the context
        // information.
        ThreadContext* currentContext = core.localThreadContexts[currentIndex];
        if (currentIndex == core.highestOccupiedContext + 1) {
            // Check whether we need to increment core.highestOccupiedContext or
            // reset currentIndex.
            if (currentContext->wakeupTimeInCycles !=
                ThreadContext::UNOCCUPIED) {
                core.highestOccupiedContext++;
            } else {
                currentIndex = 0;
                // Reload the currentContext since we have reset currentIndex.
                currentContext = core.localThreadContexts[currentIndex];
            }
        }
        if (currentIndex == 0) {
            // Update stats and check for arbiter preemption; done once per
            // cycle over all contexts on this core.
            checkForArbiterRequest();
            dispatchIterationStartCycles = Cycles::rdtsc();
            // Flush counters to keep times up to date
            idleTimeTracker.updatePerfStats();

            // Check for termination
            if (shutdown) {
                swapcontext(&kernelThreadStacks[core.id],
                            &core.loadedContext->sp);
            }

            PerfStats::threadStats->weightedLoadedCycles +=
                IdleTimeTracker::numThreadsRan *
                (dispatchIterationStartCycles -
                 IdleTimeTracker::lastDispatchIterationStart);

            IdleTimeTracker::numThreadsRan = 0;
            IdleTimeTracker::lastDispatchIterationStart =
                dispatchIterationStartCycles;
        }

        // Decide whether we can run the current thread.
        if (dispatchIterationStartCycles >=
            currentContext->wakeupTimeInCycles) {
            core.nextCandidateIndex = currentIndex + 1;

            if (currentContext == core.loadedContext) {
                core.loadedContext->wakeupTimeInCycles = ThreadContext::BLOCKED;
                IdleTimeTracker::numThreadsRan++;
                return;
            }
            void** saved = &core.loadedContext->sp;
            core.loadedContext = currentContext;

            // Flush the idle cycle counter before a context switch because
            // switching to a fresh (previously unused) context will cause
            // dispatch to be called from the top again before this
            // invocation returns. This is problematic because it resets
            // dispatchStartCycles (used for computing idle cycles) but not
            // lastTotalCollectionTime (used for computing total cycles).
            idleTimeTracker.updatePerfStats();
            swapcontext(&core.loadedContext->sp, saved);
            // After the old context is swapped out above, this line executes
            // in the new context.
            originalContext->wakeupTimeInCycles = ThreadContext::BLOCKED;
            IdleTimeTracker::numThreadsRan++;
            return;
        }
    }
}

/**
 * Block the current thread until another thread invokes join() with the
 * current thread's ThreadId.
 */
void
block() {
    dispatch();
}

/**
 * Perform a atomic compare_exchange operation on a 64-bit value, returning the
 * old value. This is useful for performing atomic CAS operations on non-atomic
 * variable.
 *
 * \param target
 *    The memory address to attempt to CAS.
 * \param test
 *    If the target is equal to this value, then it will be set to newValue.
 * \param newValue
 *    The value to set if the test succeeds.
 */
uint64_t
compareExchange(volatile uint64_t* target, uint64_t test, uint64_t newValue) {
    __asm__ __volatile__("lock; cmpxchgq %0,%1"
                         : "=r"(newValue), "=m"(*target), "=a"(test)
                         : "0"(newValue), "2"(test));
    return test;
}

/**
 * Make the thread referred to by ThreadId runnable.
 * If one thread exits and another is created between the check and the setting
 * of the wakeup flag, this signal will result in a spurious wake-up.
 * If this method is invoked on a currently running thread, it will have the
 * effect of causing the thread to immediately unblock the next time it blocks.
 *
 * \param id
 *     The id of the thread to signal.
 */
void
signal(ThreadId id) {
    // Speculatively assume that that the thread being signaled is in the
    // BLOCKED state, and retry the CAS if it is not. This approach avoids
    // first taking a cache miss to read and then performing a CAS in the case
    // when the target thread is actually BLOCKED.
    // Experiments show that this approach can save 40 ns compared to explicitly
    // reading wakeupTimeInCycles and then performing the CAS.
    //
    // When explicitly reading, we pay:
    //     1 cache miss to read on the signaler core.
    //     1 cache invalidate other caches during the CAS.
    //     1 cache miss to read on the target core.
    // Under the CAS-first approach, if the wakeupTimeInCycles == BLOCKED, we
    // pay:
    //     1 cache invalidate other caches during the CAS.
    //     1 cache miss to read on the target core.
    // This method uses CAS rather than a blind write to avoid accidentally
    // signalling a thread that just exited, which might cause us to attempt to
    // execute on an empty ThreadContext
    uint64_t oldWakeupTime = ThreadContext::BLOCKED;
    uint64_t newValue = 0L;
    oldWakeupTime = compareExchange(&id.context->wakeupTimeInCycles,
                                    oldWakeupTime, newValue);

    // The original value was not BLOCKED, so we try again with the true
    // original value, unless the target is already runnable or UNOCCUPIED.
    // This typically happens if the target thread was sleeping rather than
    // blocked.
    if (oldWakeupTime != ThreadContext::BLOCKED &&
        oldWakeupTime != ThreadContext::UNOCCUPIED && oldWakeupTime != 0L) {
        compareExchange(&id.context->wakeupTimeInCycles, oldWakeupTime,
                        newValue);
    }
    // Raise the priority of the newly awakened thread except the UNOCCUPIED.
    if (oldWakeupTime != ThreadContext::UNOCCUPIED &&
        id.context->coreId != static_cast<uint8_t>(~0)) {
        *allHighPriorityThreads[id.context->coreId] |=
            (1L << id.context->idInCore);
    }
}

/**
 * Block the current thread until the thread identified by id finishes its
 * execution.
 *
 * \param id
 *     The id of the thread to join. This id must be a valid return value from
 *     Arachne::createThread, and must not be equal to Arachne::NullThread.
 */
void
join(ThreadId id) {
    std::unique_lock<SpinLock> joinGuard(id.context->joinLock);
    // Thread has already exited.
    if (id.generation != id.context->generation)
        return;
    id.context->joinCV.wait(joinGuard);
}

/**
 * This function must be called by the main application thread and will block
 * until Arachne is terminated via a call to shutDown().
 *
 * Upon termination, this function tears down all state created by init,
 * and restores the state of the system to the time before init is
 * called.
 */
void
waitForTermination() {
    mainThreadDestroy();
    for (size_t i = 0; i < kernelThreads.size(); i++) {
        kernelThreads[i].join();
    }

    // We now assume that all threads are done executing.
    PerfUtils::Util::serialize();

    for (size_t i = 0; i < occupiedAndCount.size(); i++) {
        free(occupiedAndCount[i]);

        for (int k = 0; k < maxThreadsPerCore; k++) {
            free(allThreadContexts[i][k]->stack);
            allThreadContexts[i][k]->joinLock.~SpinLock();
            allThreadContexts[i][k]->joinCV.~ConditionVariable();
            free(allThreadContexts[i][k]);
        }
        delete[] allThreadContexts[i];
    }

    kernelThreads.clear();
    kernelThreadStacks.clear();

    allThreadContexts.clear();
    occupiedAndCount.clear();
    pinnedContexts.clear();
    allHighPriorityThreads.clear();
    PerfUtils::Util::serialize();
    coreArbiter->reset();
    delete corePolicy;
    corePolicy = NULL;
    initialized = false;
}

/**
 * This function parses out the arguments intended for the thread library from
 * a command line, and adjusts the values of argc and argv to eliminate the
 * arguments that the thread library consumed.
 */
void
parseOptions(int* argcp, const char** argv) {
    if (argcp == NULL)
        return;

    int argc = *argcp;

    struct OptionSpecifier {
        // The string that the user uses after `--`.
        const char* optionName;
        // The id for the option that is returned when it is recognized.
        int id;
        // Does the option take an argument?
        bool takesArgument;
    } optionSpecifiers[] = {{"minNumCores", 'c', true},
                            {"maxNumCores", 'm', true},
                            {"stackSize", 's', true},
                            {"enableArbiter", 'a', true},
                            {"disableLoadEstimation", 'd', false},
                            {"coreArbiterSocketPath", 'p', true}};
    const int UNRECOGNIZED = ~0;

    int i = 1;
    while (i < argc) {
        if (argv[i][0] != '-' || argv[i][1] != '-') {
            i++;
            continue;
        }
        const char* optionName = argv[i] + 2;
        int optionId = UNRECOGNIZED;
        const char* optionArgument = NULL;

        for (size_t k = 0;
             k < sizeof(optionSpecifiers) / sizeof(OptionSpecifier); k++) {
            const char* candidateName = optionSpecifiers[k].optionName;
            bool needsArg = optionSpecifiers[k].takesArgument;
            if (strncmp(candidateName, optionName, strlen(candidateName)) ==
                0) {
                if (needsArg) {
                    if (i + 1 >= argc) {
                        ARACHNE_LOG(ERROR, "Missing argument to option %s!\n",
                                    candidateName);
                        break;
                    }
                    optionArgument = argv[i + 1];
                    optionId = optionSpecifiers[k].id;
                    argc -= 2;
                    memmove(argv + i, argv + i + 2, (argc - i) * sizeof(char*));
                } else {
                    optionId = optionSpecifiers[k].id;
                    argc -= 1;
                    memmove(argv + i, argv + i + 1, (argc - i) * sizeof(char*));
                }
                break;
            }
        }
        switch (optionId) {
            case 'c':
                minNumCores = atoi(optionArgument);
                break;
            case 'm':
                maxNumCores = atoi(optionArgument);
                break;
            case 's':
                stackSize = atoi(optionArgument);
                break;
            case 'd':
                disableLoadEstimation = true;
                break;
            case 'a':
                useCoreArbiter = (0 != atoi(optionArgument));
                break;
            case 'p':
                coreArbiterSocketPath = optionArgument;
                break;
            case UNRECOGNIZED:
                i++;
        }
    }
    *argcp = argc;
}

ThreadContext::ThreadContext(uint8_t idInCore)
    : stack(NULL),
      sp(NULL),
      generation(1),
      joinLock(),
      joinCV(),
      coreId(CORE_UNASSIGNED),
      originalCoreId(coreId),
      idInCore(idInCore),
      threadInvocation(),
      wakeupTimeInCycles(threadInvocation.wakeupTimeInCycles) {
    wakeupTimeInCycles = ThreadContext::UNOCCUPIED;
    // Allocate memory here so we can error-check the return value of malloc.
    stack = alignedAlloc(stackSize, PAGE_SIZE);
    if (stack == NULL) {
        abort();
    }
}

/**
 * This method initializes a stack so that when the dispatcher context switches
 * back to this Arachne thread, the thread will resume execution at the
 * beginning of the schedulerMainLoop method.
 */
void
ThreadContext::initializeStack() {
    sp = reinterpret_cast<char*>(stack) + stackSize - 2 * sizeof(void*);

    // Immediately before schedulerMainLoop gains control, we want the
    // stack to look like this, so that the swapcontext call will
    // transfer control to schedulerMainLoop.
    //           +-----------------------+
    //           |                       |
    //           +-----------------------+
    //           |     Return Address    |
    //           +-----------------------+
    //     sp->  |       Registers       |
    //           +-----------------------+
    //           |                       |
    //           |                       |
    //
    // Set up the stack so that the first time we switch context to
    // this thread, we enter schedulerMainLoop.
    *reinterpret_cast<void**>(sp) = reinterpret_cast<void*>(schedulerMainLoop);

    /**
     * Decrement the stack pointer by the amount of space needed to
     * store the registers in swapcontext.
     */
    sp = reinterpret_cast<char*>(sp) - SPACE_FOR_SAVED_REGISTERS;

    /**
     * Set the stack canary value to detect stack overflows.
     */
    *reinterpret_cast<uint64_t*>(stack) = STACK_CANARY;
}

/**
 * Set the core policy for Arachne, if the application wants a policy other
 * than the default. This function should be invoked before Arachne::init(),
 * and the object passed in is owned by Arachne after this function returns.
 *
 * \param arachneCorePolicy
 *    A pointer to the CorePolicy that Arachne will use.  The CorePolicy
 *    controls thread allocation to cores.
 */
void
setCorePolicy(CorePolicy* arachneCorePolicy) {
    if (initialized) {
        ARACHNE_LOG(ERROR,
                    "Attempting to set core policy after Arachne::init has "
                    "already been invoked");
        abort();
    }
    if (corePolicy != NULL)
        delete corePolicy;
    corePolicy = arachneCorePolicy;
}

/**
 * Get the current core policy used by Arachne. This should be used only for
 * testing.
 */
CorePolicy*
getCorePolicy() {
    return corePolicy;
}

/**
 * This function sets up state needed by the thread library, and must be
 * invoked before any other function in the thread library is invoked. It is
 * undefined behavior to invoke other Arachne functions before this one.
 *
 * Arachne will take configuration options from the command line specified by
 * argc and argv, and then update the values of argv and argc to reflect the
 * remaining arguments.
 *
 * Here are the current available options.
 *
 *     --minNumCores
 *        The minimum number of cores the application should use. This number
 *        must be at least one higher than the number of cores which are
 *        permanently exclusive.
 *     --maxNumCores
 *        The largest number of core the appliation may use
 *     --stackSize
 *        The size of each user stack.
 *
 * \param argcp
 *    The pointer to argc, the number of arguments passed to the application.
 *    This pointer will be used to update argc after Arachne has consumed its
 *    arguments.
 * \param argv
 *    The pointer to the command line argument array, which will be modiifed to
 *    remove the options that Arachne recognizes.
 */
void
init(int* argcp, const char** argv) {
    if (initialized)
        return;

    parseOptions(argcp, argv);

    if (!useCoreArbiter) {
        coreArbiter = ArbiterClientShim::getInstance();
    } else if (coreArbiterSocketPath.empty()) {
        coreArbiter = CoreArbiterClient::getInstance();
    } else {
        coreArbiter = CoreArbiterClient::getInstance(coreArbiterSocketPath);
    }

    volatile uint32_t numHardwareCores = std::thread::hardware_concurrency();
    // CoreArbiter reserves 1 core to run non-Arachne threads.
    volatile uint32_t hardwareCoresAvailable = numHardwareCores - 1;
    if (!useCoreArbiter)
        hardwareCoresAvailable++;
    if (minNumCores == 0)
        minNumCores = 1;
    if (maxNumCores == 0)
        maxNumCores = hardwareCoresAvailable;

    minNumCores = std::min(minNumCores, hardwareCoresAvailable);
    maxNumCores = std::max(minNumCores, maxNumCores);

    if (maxNumCores > hardwareCoresAvailable) {
        ARACHNE_LOG(WARNING,
                    "maxNumCores %d is greater than the number of available "
                    "hardware cores %d.",
                    maxNumCores, hardwareCoresAvailable);
    }

    if (corePolicy == NULL) {
        corePolicy = new DefaultCorePolicy(maxNumCores, !disableLoadEstimation);
    }

    lastTotalCollectionTime.resize(numHardwareCores);
    // Create enough data structures to account for every core in the system.
    occupiedAndCount.resize(numHardwareCores);
    pinnedContexts.resize(numHardwareCores);
    allHighPriorityThreads.resize(numHardwareCores);
    allThreadContexts.resize(numHardwareCores);
    for (unsigned int i = 0; i < numHardwareCores; i++) {
        occupiedAndCount[i] =
            reinterpret_cast<std::atomic<Arachne::MaskAndCount>*>(
                alignedAlloc(sizeof(std::atomic<MaskAndCount>)));
        memset(occupiedAndCount[i], 0, sizeof(std::atomic<MaskAndCount>));

        // Allocate all the thread contexts and stacks
        ThreadContext** contexts = new ThreadContext*[maxThreadsPerCore];
        for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
            contexts[k] = reinterpret_cast<ThreadContext*>(
                alignedAlloc(sizeof(ThreadContext)));
            new (contexts[k]) ThreadContext(k);
        }
        allThreadContexts[i] = contexts;

        coreIdleSemaphores.push_back(new ::Semaphore);
    }

    // Allocate space to store all the original kernel pointers
    kernelThreadStacks.resize(numHardwareCores);
    shutdown = false;

    // Ensure that data structure and stack allocation completes before we
    // begin to use it in a new thread.
    PerfUtils::Util::serialize();

    // Request the mininum number of cores.
    std::vector<uint32_t> coreRequest({minNumCores, 0, 0, 0, 0, 0, 0, 0});
    coreArbiter->setRequestedCores(coreRequest);

    // Note that the main thread is not part of the thread pool.
    for (unsigned int i = 0; i < maxNumCores; i++) {
        // These threads are started with threadMain instead of
        // schedulerMainLoop because we want schedulerMainLoop to run on a user
        // stack rather than a kernel-provided stack. This enables us to run
        // the first user thread without a context switch.
        kernelThreads.emplace_back(threadMain);
    }

    // Block until minNumCores is active, per the application's requirements.
    while (numActiveCores != minNumCores)
        usleep(1);

    mainThreadInit();
    // Only consider Arachne initialized if we've successfully parsed options
    // allocated all resources, and connected to the CoreArbiter
    initialized = true;
}

/**
 * Set up just enough state to allow the current thread to invoke Arachne
 * functions without being on an Arachne core. It is automatically invoked in
 * the caller of Arachne::init(). This function sets up ThreadContexts,
 * occupiedAndCount, and a thread-local copy of PerfStats, all of which must be
 * destroyed by mainThreadDestroy().
 */
void
mainThreadInit() {
    initializeCore(&core);
    core.localOccupiedAndCount =
        reinterpret_cast<std::atomic<Arachne::MaskAndCount>*>(
            alignedAlloc(sizeof(std::atomic<MaskAndCount>)));
    for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
        // It is important to re-initialize stacks here because some of the
        // contexts may be in the middle of dispatch calls from
        // schedulerMainLoop and switching to them will a spurious return from
        // dispatch at the start of schedulerMainLoop, which it is expensive to
        // check for.
        core.localThreadContexts[k]->initializeStack();
    }
    core.loadedContext = *core.localThreadContexts;
    core.loadedContext->wakeupTimeInCycles = ThreadContext::BLOCKED;
    *core.localOccupiedAndCount = {1, 1};
    PerfStats::threadStats = std::unique_ptr<PerfStats>(new PerfStats());
}

/**
 * Tear down the state set up by mainThreadInit().
 */
void
mainThreadDestroy() {
    deinitializeCore(&core);
    PerfStats::threadStats = NULL;
}

/**
 * This call will cause all Arachne threads to terminate, and cause
 * waitForTermination() to return.
 *
 * It is typically used only for an application's unit tests, where the global
 * teardown function in the unit test would call Arachne::shutDown() followed
 * immediately by Arachne::waitForTermination().
 *
 * This function can be called from any Arachne or non-Arachne thread.
 */
void
shutDown() {
    // Tell all the kernel threads to terminate at the first opportunity.
    shutdown = true;

    // Unblock all cores so they can shut down and be joined.
    std::vector<uint32_t> coreRequest({maxNumCores, 0, 0, 0, 0, 0, 0, 0});
    coreArbiter->setRequestedCores(coreRequest);
}

/**
 * Attempt to acquire this resource and block if it is not available.
 */
void
SleepLock::lock() {
    std::unique_lock<SpinLock> guard(blockedThreadsLock);
    if (owner == NULL) {
        owner = core.loadedContext;
        return;
    }
    blockedThreads.push_back(getThreadId());
    guard.unlock();
    while (true) {
        // Spurious wake-ups can happen due to signalers of past inhabitants of
        // this core.loadedContext.
        dispatch();
        blockedThreadsLock.lock();
        if (owner == core.loadedContext) {
            blockedThreadsLock.unlock();
            break;
        }
        blockedThreadsLock.unlock();
    }
}

/**
 * Attempt to acquire this resource once.
 * \return
 *    Whether or not the acquisition succeeded.
 */
bool
SleepLock::try_lock() {
    std::lock_guard<SpinLock> guard(blockedThreadsLock);
    if (owner == NULL) {
        owner = core.loadedContext;
        return true;
    }
    return false;
}

/** Release resource. */
void
SleepLock::unlock() {
    blockedThreadsLock.lock();
    if (blockedThreads.empty()) {
        owner = NULL;
        blockedThreadsLock.unlock();
        return;
    }
    owner = blockedThreads.front().context;
    signal(blockedThreads.front());
    blockedThreads.pop_front();
    blockedThreadsLock.unlock();
}

ConditionVariable::ConditionVariable() : blockedThreads() {}

ConditionVariable::~ConditionVariable() {}

/**
 * Awaken one of the threads waiting on this condition variable.
 * The caller must hold the mutex that waiting threads held when they called
 * wait().
 */
void
ConditionVariable::notifyOne() {
    if (blockedThreads.empty())
        return;
    ThreadId awakenedThread = blockedThreads.front();
    blockedThreads.pop_front();
    signal(awakenedThread);
}

/**
 * Awaken all of the threads waiting on this condition variable.
 * The caller must hold the mutex that waiting threads held when they called
 * wait().
 */
void
ConditionVariable::notifyAll() {
    while (!blockedThreads.empty())
        notifyOne();
}

// Constructor
Semaphore::Semaphore()
    : countProtector("countprotector", false), countWaiter(), count(0) {}

/**
 * Change this Semaphore to a fully locked state.
 */
void
Semaphore::reset() {
    std::unique_lock<decltype(countProtector)> lock(countProtector);
    count = 0;
}

/**
 * If there are threads waiting on this semaphore, wake up one of them.
 * Otherwise, the next thread to wait on this semaphore will immediately
 * awaken.
 */
void
Semaphore::notify() {
    std::unique_lock<decltype(countProtector)> lock(countProtector);
    ++count;
    countWaiter.notifyOne();
}

/**
 * Block until another thread notifies.
 */
void
Semaphore::wait() {
    std::unique_lock<decltype(countProtector)> lock(countProtector);
    while (!count)  // Handle spurious wake-ups.
        countWaiter.wait(lock);
    --count;
}

/**
 * Attempt to acquire the resource if it is available.
 * \return
 *     The return value is true iff the resource represented by this
 *     Semaphor was successfully acquired.
 */
bool
Semaphore::try_wait() {
    std::unique_lock<decltype(countProtector)> lock(countProtector);
    if (count) {
        --count;
        return true;
    }
    return false;
}

// Constructor
IdleTimeTracker::IdleTimeTracker() {
    dispatchStartCycles = Cycles::rdtsc();
    // Initialize here instead of in threadMain, since this is the first
    // opportunity to actually accumulate either idle cycles or do real
    // work.
    if (!lastTotalCollectionTime)
        lastTotalCollectionTime = dispatchStartCycles;
}

// Invoke this method to insert the current counts for idle time and total
// time into PerfStats.
// This method is used in dispatch() in place of destruction followed by
// construction to avoid leaking idle cycles.
void
IdleTimeTracker::updatePerfStats() {
    uint64_t currentTime = Cycles::rdtsc();
    PerfStats::threadStats->totalCycles +=
        currentTime - lastTotalCollectionTime;
    PerfStats::threadStats->idleCycles += currentTime - dispatchStartCycles;
    lastTotalCollectionTime = currentTime;
    dispatchStartCycles = currentTime;
}

IdleTimeTracker::~IdleTimeTracker() {
    uint64_t currentTime = Cycles::rdtsc();
    PerfStats::threadStats->totalCycles +=
        currentTime - lastTotalCollectionTime;
    PerfStats::threadStats->idleCycles += currentTime - dispatchStartCycles;
    lastTotalCollectionTime = currentTime;
}

// Constructor
NestedDispatchDetector::NestedDispatchDetector() {
    if (unlikely(dispatchRunning)) {
        ARACHNE_LOG(ERROR, "Nested invocation to dispatch thread!");
        // Abort with a backtrace
        ARACHNE_BACKTRACE(ERROR);
        abort();
    }

    dispatchRunning = true;
}

// Destructor
NestedDispatchDetector::~NestedDispatchDetector() {
    // It is correct for a different Arachne thread to have cleared the
    // flag already.
    dispatchRunning = false;
}

// This method is invoked to prevent a false alarm when schedulerMainLoop calls
// dispatch() the first time.
void
NestedDispatchDetector::clearDispatchFlag() {
    dispatchRunning = false;
}

/**
 * Change the target of the error stream, allowing redirection to an
 * application's log.
 */
void
setErrorStream(FILE* stream) {
    errorStream = stream;
}

/*
 * If the Core Arbiter asks the Arachne runtime to yield the current core, this
 * function shall begin the process of descheduling a core.
 */
void
descheduleCore() {
    if (core.coreDeschedulingScheduled)
        return;
    // Create a thread on the this core to handle the actual core release,
    // since we are currently borrowing an arbitrary context and should not
    // hold it for too long.
    int coreId = core.id;
    core.coreDeschedulingScheduled = true;
    corePolicy->coreUnavailable(coreId);
    if (createThreadOnCore(coreId, releaseCore) == NullThread) {
        ARACHNE_LOG(WARNING,
                    "Failed to create a thread on core %d for core release! "
                    "Might be an exclusive core?\n",
                    coreId);
        // Since we failed to initiate the core release, Arachne still consider
        // the core owned, as should the corePolicy.
        core.coreDeschedulingScheduled = false;
        corePolicy->coreAvailable(coreId);
    }
}

/**
 * This function tells the CoreArbiter the currently desired number of cores.
 */
void
setCoreCount(uint32_t desiredNumCores) {
    if (desiredNumCores < minNumCores || desiredNumCores > maxNumCores)
        return;

    ARACHNE_LOG(NOTICE, "Attempting to change number of cores: %u --> %u\n",
                numActiveCores.load(), desiredNumCores);
    std::vector<uint32_t> coreRequest({desiredNumCores, 0, 0, 0, 0, 0, 0, 0});
    coreArbiter->setRequestedCores(coreRequest);
}

/**
 * Detect requests for cores from the core arbiter.
 */
void
checkForArbiterRequest() {
    // The Core Arbiter wants us to release the core running this check.
    if (coreArbiter->mustReleaseCore())
        descheduleCore();
}

/**
 * After this function returns, threads may no longer be added to the target
 * core. This function can be invoked from any thread on any core. It is
 * invoked by CorePolicys to block creations when cores transition
 * between thread classes.
 */
void
preventCreationsToCore(int coreId) {
    MaskAndCount originalMask = *occupiedAndCount[coreId];

    // It is an error if the core we are migrating to is already exclusive.
    if (originalMask.numOccupied > maxThreadsPerCore) {
        ARACHNE_LOG(ERROR,
                    "preventCreationsToCore: Occupied = %lu, numOccupied = %lu "
                    "on core %d\n",
                    originalMask.occupied, originalMask.numOccupied, coreId);
        abort();
    }

    // Block future creations on core
    MaskAndCount targetOccupiedAndCount;
    MaskAndCount blockedOccupiedAndCount;
    bool success = false;
    do {
        targetOccupiedAndCount = *occupiedAndCount[coreId];
        blockedOccupiedAndCount = targetOccupiedAndCount;
        blockedOccupiedAndCount.numOccupied = MaskAndCount::EXCLUSIVE;
        success = occupiedAndCount[coreId]->compare_exchange_strong(
            targetOccupiedAndCount, blockedOccupiedAndCount);
    } while (!success);

    // Wait out creations that finished CASing before we blocked creations
    bool pendingCreation;
    do {
        pendingCreation = false;
        for (int i = 0; i < maxThreadsPerCore; i++) {
            // It is safe to use targetOccupiedAndCount here, because the
            // success of the CAS indicates that all subsequent attempts to CAS
            // should have failed.
            // There is no race with completions here because no other thread
            // can be running on this core since we are running.
            if (((targetOccupiedAndCount.occupied >> i) & 1) &&
                core.localThreadContexts[i]->wakeupTimeInCycles ==
                    ThreadContext::UNOCCUPIED) {
                pendingCreation = true;
                break;
            }
        }
    } while (pendingCreation);
}

/**
 * Remove all threads from the target core (with the exception of the caller),
 * and migrate them into outputCores. This function can only be run from the
 * core that we are removing threads from. The parameter outputCores must not
 * include the current core.
 *
 * \param outputCores
 *     Collection of cores where threads are migrated to.
 */
void
migrateThreadsFromCore() {
    preventCreationsToCore(core.id);

    std::lock_guard<SpinLock> _(coreExclusionMutex);

    // Start migration of remaining threads.
    MaskAndCount blockedOccupiedAndCount = *core.localOccupiedAndCount;

    // Migrate off all threads other than the current one.  Round robin among
    // cores because these are likely long-running threads.
    int failureCount = 0;
    for (uint8_t i = 0; i < maxThreadsPerCore; i++) {
        if (core.localThreadContexts[i] == core.loadedContext) {
            // Skip over ourselves
            continue;
        }
        // Choose a victim core that we will pawn our work on.
        if ((blockedOccupiedAndCount.occupied >> i) & 1) {
            int threadClass = core.localThreadContexts[i]->threadClass;
            CorePolicy::CoreList outputCores =
                corePolicy->getCores(threadClass);
            if (outputCores.size() == 0) {
                ARACHNE_LOG(ERROR,
                            "No available cores to migrate threads to for "
                            "threadclass %d.",
                            threadClass);
                abort();
            }
            int coreId = chooseCore(outputCores);

            bool success = false;
            uint8_t index;
            do {
                // Each iteration through this loop makes one attempt to
                // enqueue the task to the specified core. Multiple iterations
                // are required only if there is contention for the core's
                // state variables.
                MaskAndCount slotMap = *occupiedAndCount[coreId];
                MaskAndCount oldSlotMap = slotMap;

                // Skip this core since it might be an exclusive or fully
                // loaded.
                if (slotMap.numOccupied >= maxThreadsPerCore) {
                    success = false;
                    break;
                }

                // Search for a non-occupied slot and attempt to reserve the
                // slot
                index = 0;
                while (((slotMap.occupied | *pinnedContexts[coreId]) &
                        (1L << index)) &&
                       index < maxThreadsPerCore)
                    index++;

                // Not able to find a context, likely because unoccupied
                // contexts were pinned.
                if (index == maxThreadsPerCore) {
                    success = false;
                    break;
                }

                slotMap.occupied =
                    (slotMap.occupied | (1L << index)) & 0x00FFFFFFFFFFFFFF;
                slotMap.numOccupied++;
                success = occupiedAndCount[coreId]->compare_exchange_strong(
                    oldSlotMap, slotMap);
            } while (!success);

            if (success) {
                // Now that we have found a slot, we can clear our bit.
                blockedOccupiedAndCount.occupied &=
                    ~(1L << i) & 0x00FFFFFFFFFFFFFF;

                // At this point we've reserved a spot on the target, and now
                // we swap the contexts. We correct the idInCore before
                // swapping, to ensure that the correct slot is cleared in
                // occupiedAndCount on the target core.
                allThreadContexts[coreId][index]->idInCore = i;
                core.localThreadContexts[i]->idInCore = index;

                allThreadContexts[coreId][index]->coreId =
                    static_cast<uint8_t>(core.id);
                core.localThreadContexts[i]->coreId =
                    static_cast<uint8_t>(coreId);

                ThreadContext* contextToMigrate =
                    allThreadContexts[coreId][index];
                allThreadContexts[coreId][index] = core.localThreadContexts[i];
                core.localThreadContexts[i] = contextToMigrate;
            } else {
                ARACHNE_LOG(
                    WARNING,
                    "Failed to find a core to migrate thread of class %d.",
                    threadClass);
                failureCount++;
                if (failureCount >= MAX_MIGRATION_RETRIES) {
                    abort();
                }
                i--;
            }
        }
    }

    // Sanity checking that we are the only thread left on this core.
    int count = 0;
    for (int i = 0; i < maxThreadsPerCore; i++)
        if (blockedOccupiedAndCount.occupied & (1L << i))
            count++;
    if (count != 1) {
        ARACHNE_LOG(ERROR,
                    "Failed to migrate threads off core; number of threads "
                    "remaining on core %d\n",
                    count);
        abort();
    }

    // Update core.localOccupiedAndCount to a consistent state before exiting.
    // At this point, creations should have already been blocked, and
    // completions cannot occur because we are running, so we can just directly
    // assign.
    *core.localOccupiedAndCount = blockedOccupiedAndCount;
}

/**
 * This function runs on a core immediately before it is deallocated, and is
 * responsible for waiting out and then migrating running threads other than
 * itself. By ensuring that the core is busy running this thread, we ensure
 * that all other threads' contexts on this core are saved.
 */
void
releaseCore() {
    // Remove all other threads from this core.
    migrateThreadsFromCore();
    core.coreReadyForReturnToArbiter = true;
}

/*
 * Put the core into the most quiescent possible state to minimize performance
 * interference with the hypertwin. Note that a core only becomes unidled if
 * there have been at least as many calls to unidleCore  as there are idleCore.
 *
 * \param coreId
 *     The coreId of the core that will idle
 */
void
idleCore(int coreId) {
    if (createThreadOnCore(coreId, idleCorePrivate) == NullThread) {
        ARACHNE_LOG(ERROR, "Error creating idleCorePrivate thread\n");
    }
}

/*
 * This method is invoked on the core being idled; puts the core into a deep
 * sleep.
 *
 * \param coreId
 *     The coreId of the core that will idle
 */
void
idleCorePrivate() {
    // Our experiments show that Linux will put the core to sleep.
    coreIdleSemaphores[core.id]->wait();
}

/*
 * Unidle an core. If it was not idle, the next call to idleCore will not idle
 * the core.
 *
 * \param coreId
 *     The coreId of the core that will be unidled.
 */
void
unidleCore(int coreId) {
    coreIdleSemaphores[coreId]->notify();
}

/**
 * This method puts the given core into a state such that no threads are
 * running on it and only a single thread can be scheduled onto it.
 */
void
prepareForExclusiveUse(int coreId) {
    ThreadId migrationThread =
        createThreadOnCore(coreId, migrateThreadsFromCore);
    // The current thread is a non-Arachne thread.
    if (core.id == -1) {
        // Polling for completion is a short-term hack until we figure out a
        // good story for joining Arachne threads from non-Arachne threads.
        while (Arachne::occupiedAndCount[coreId]->load().occupied)
            usleep(10);
    } else {
        Arachne::join(migrationThread);
    }

    // Prepare this core for scheduling exclusively.
    // By setting numOccupied to one less than the maximium number of threads
    // per core, we ensure that only one thread gets scheduled onto this core.
    *occupiedAndCount[coreId] = {0, maxThreadsPerCore - 1};
}

/**
 * Look for any core which is entirely unoccupied, remove it from the given
 * CoreList and return its Id.
 */
int
findAndClaimUnusedCore(CorePolicy::CoreList* cores) {
    for (uint32_t i = 0; i < cores->size(); i++) {
        int coreId = cores->get(i);
        MaskAndCount slotMap = *occupiedAndCount[coreId];
        if (slotMap.occupied == 0) {
            // Attempt to reclaim this core with a CAS. Only move back
            // to sharedCores if we succeed.
            MaskAndCount oldSlotMap = slotMap;
            slotMap.numOccupied = maxThreadsPerCore;
            if (occupiedAndCount[coreId]->compare_exchange_strong(oldSlotMap,
                                                                  slotMap)) {
                cores->remove(i);
                *lastTotalCollectionTime[coreId] = 0;
                *occupiedAndCount[coreId] = {0, 0};
                return coreId;
            }
        }
    }
    return -1;
}

}  // namespace Arachne
