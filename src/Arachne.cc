/* Copyright (c) 2015-2017 Stanford University
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR(S) DISCLAIM ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL AUTHORS BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <stdio.h>
#include <thread>
#include "PerfUtils/TimeTrace.h"
#include "PerfUtils/Util.h"
#include "CoreArbiter/CoreArbiterClient.h"

#include "Arachne.h"
#include "ArbiterClientShim.h"

namespace Arachne {

// Change 0 -> 1 in the following line to compile detailed time tracing in
// this file.
#define TIME_TRACE 0

using PerfUtils::Cycles;
using PerfUtils::TimeTrace;
using CoreArbiter::CoreArbiterClient;

/**
  * This variable prevents multiple initializations of the library, but does
  * not protect against the user calling Arachne functions without initializing
  * the library, which results in undefined behavior.
  */
bool initialized = false;

// Used to allow redirection of error messages generated by the library.
FILE* errorStream = stderr;

std::function<void()> initCore = nullptr;

// The following configuration options can be passed into init.

/**
  * The minimum number of cores this application needs to run. This number must
  * be at least one higher than the number of cores which are permanently
  * exclusive.
  *
  * If this is set higher than the number of physical cores, the kernel will
  * multiplex, which is usually undesirable except when running unit tests on a
  * single-core system.
  */
volatile uint32_t minNumCores;

/**
  * This tracks the actual number of unblocked cores in the system.
  */
std::atomic<uint32_t> numActiveCores;

/**
  * Number of outstanding requests to yield cores back to the arbiter.
  */
uint32_t coreReleaseRequestCount = 0;

/**
  * Track the number of exclusive cores so that we can calculate utilization
  * accurately among the remaining cores.
  */
std::atomic<uint32_t> numExclusiveCores;

/**
  * The largest number of cores that Arachne is permitted to utilize.  It is an
  * invariant that maxNumCores >= numActiveCores, but if the user explicitly
  * sets both then numActiveCores will push maxNumCores up to satisfy the
  * invariant.
  */
volatile uint32_t maxNumCores;

/**
  * Protect state related to changes in the number of cores, and prevents
  * multiple threads from simultaneously attempting to change the number of
  * cores.
  */
SpinLock coreChangeMutex(false);

/**
  * The mutex above cannot be held across the scaling up and down of the number
  * of cores, so this variable is set when there is a core change in effect, to
  * prevent multiple core changes from occurring simultaneously.
  */
std::atomic<bool> coreChangeActive;

/**
  * Used to ensure that only one thread attempts to become exclusive or shared
  * at a time. This protects against a thread being migrated while it is
  * halfway through makeExclusiveOnCore and making incorrect assumptiosn about
  * the core it is currently on.
  */
SleepLock coreExclusionMutex;

/**
  * Configurable maximum stack size for all threads.
  */
int stackSize = 1024 * 1024;

/**
  * Keep track of the kernel threads we are running so that we can join them on
  * destruction. Also, store a pointer to the original stacks to facilitate
  * switching back and joining.
  */
std::vector<std::thread> kernelThreads;
std::vector<void*> kernelThreadStacks;

/**
  * Alert the kernel threads that they should exit immediately. This is used
  * only for testing.
  */
volatile bool shutdown;

/**
  * The collection of possibly runnable contexts for each kernel thread.
  */
std::vector<ThreadContext**> allThreadContexts;

/**
  * See documentation for MaskAndCount.
  */
std::vector<std::atomic<MaskAndCount> * > occupiedAndCount;

/**
  * This table maps a contiguous range of virtual core ID's to indices into an
  * immutable table pairing each ThreadContext** with its corresponding
  * MaskAndCount structure.
  */
int* virtualCoreTable;

/**
  * Setting a jth bit in the ith element of this vector indicates that the
  * priority of the thread living at index j on core i is temporarily raised.
  */
std::vector< std::atomic<uint64_t> *> publicPriorityMasks;

/**
  * The period in ns over which we measure before deciding to reduce the number
  * of cores we use.
  */
const uint64_t MEASUREMENT_PERIOD = 50 * 1000 * 1000;

/**
  * Arachne will attempt to increase the number of cores if the idle core
  * fraction (computed as idlePercentage * numSharedCores) is less than this
  * number.
  */
double maxIdleCoreFraction = 0.1;

/**
  * Arachne will attempt to increase the number of cores if the load factor
  * increase beyond this threshold.
  */
double loadFactorThreshold = 1.0;

/**
  * Only here to satisfy compiling benchmark.
  */
double maxUtilization = 0.9;

/**
  * Save the core fraction at which we ramped up based on load factor, so we
  * can decide whether to ramp down.
  */
double *utilizationThresholds;

/**
  * The difference in load, expressed as a fraction of a core, between a
  * ramp-down threshold and the corresponding ramp-up threshold.
  */
double idleCoreFractionHysteresis = 0.2;

/**
  * The percentage of slots which are occupied that will prevent reducing the
  * number of cores.
  */
double SLOT_OCCUPANCY_THRESHOLD = 0.5;

void incrementCoreCount();
void decrementCoreCount();
void descheduleCore();
/**
  * All core-specific state that is not associated with other classes.
  */
thread_local Core core;

// BEGIN Testing-Specific Flags
bool disableLoadEstimation;
// END   Testing-Specific Flags

// Allocate storage for load-tracking pointers.
thread_local uint64_t DispatchTimeKeeper::lastTotalCollectionTime;
thread_local uint64_t DispatchTimeKeeper::dispatchStartCycles;
thread_local uint64_t DispatchTimeKeeper::lastDispatchIterationStart;
thread_local uint8_t  DispatchTimeKeeper::numThreadsRan;

#ifndef NO_ARBITER
/**
  * A handle to the CoreArbiterClient, which we use for requesting and
  * returning cores.
  */
CoreArbiterClient& coreArbiter =
    CoreArbiterClient::getInstance("/tmp/CoreArbiter/testsocket");
#else
/**
  * A handle to a fake CoreArbiterClient is a thin wrapper around a semaphor.
  */
ArbiterClientShim& coreArbiter = ArbiterClientShim::getInstance();
#endif


/**
  * Allocate a block of memory aligned at the beginning of a cache line.
  *
  * \param size
  *     The amount of memory to allocate.
  */
void*
alignedAlloc(size_t size, size_t alignment) {
    void *temp;
    int result = posix_memalign(&temp, alignment, size);
    if (result != 0) {
        ARACHNE_LOG(ERROR, "posix_memalign returned %s", strerror(result));
        exit(1);
    }
    assert((reinterpret_cast<uint64_t>(temp) & (alignment - 1)) == 0);
    return temp;
}

/**
 * Main function for a kernel thread, which roughly corresponds to a core in the
 * current design of the system.
 *
 * \param kId
 *     The kernel thread ID for the newly created kernel thread.
 */
void
threadMain() {
    if (initCore) initCore();

    for (;;) {
        coreArbiter.blockUntilCoreAvailable();
        // Prevent the use of abandoned ThreadContext which occurred as a
        // result of a shutdown request.
        if (shutdown) break;
        {
            std::lock_guard<SpinLock> _(coreChangeMutex);
            core.kernelThreadId = virtualCoreTable[numActiveCores];
            core.localOccupiedAndCount = occupiedAndCount[core.kernelThreadId];
            core.localThreadContexts = allThreadContexts[core.kernelThreadId];

            DispatchTimeKeeper::lastTotalCollectionTime = 0;
            // Clean up state from the previous thread that was using this data
            // structure.
            *core.localOccupiedAndCount = {0,0};
            *publicPriorityMasks[core.kernelThreadId] = 0;
            core.privatePriorityMask = 0;

            // This marks the point at which new thread creations may begin.
            numActiveCores++;
            ARACHNE_LOG(DEBUG, "Number of cores increased from %d to %d\n",
                    numActiveCores - 1, numActiveCores.load());
            #if TIME_TRACE
            TimeTrace::record("Core Count %d --> %d",
                    numActiveCores - 1, numActiveCores.load());
            #endif
            if (coreChangeActive) coreChangeActive = false;
            PerfStats::threadStats.numCoreIncrements++;
        }


        // Correct the ThreadContext.coreId() here to match the existing core.
        // It is valid to initialize this after incrementing numActiveCores
        // because thread creations do not touch these variables.
        for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
            core.localThreadContexts[k]->coreId =
                static_cast<uint8_t>(core.kernelThreadId);
            core.localThreadContexts[k]->initializeStack();
        }

        // Correct statistics
        DispatchTimeKeeper::numThreadsRan = 0;
        DispatchTimeKeeper::lastDispatchIterationStart = Cycles::rdtsc();

        core.loadedContext = core.localThreadContexts[0];

        // Transfers control to the Arachne dispatcher.
        // This context has been pre-initialized by init so it will "return"
        // to the schedulerMainLoop.
        // This call will return iff shutDown is called from the main thread.
        swapcontext(&core.loadedContext->sp,
                &kernelThreadStacks[core.kernelThreadId]);
        numActiveCores--;
        if (shutdown) break;
        {
            std::lock_guard<SpinLock> _(coreChangeMutex);
            ARACHNE_LOG(DEBUG, "Number of cores decreased from %d to %d\n",
                    numActiveCores + 1, numActiveCores.load());
            #if TIME_TRACE
            TimeTrace::record("Core Count %d --> %d",
                    numActiveCores + 1, numActiveCores.load());
            #endif
            coreChangeActive = false;

            // Cleanup is completed, so we can carry on with the next core
            // release if needed.
            coreReleaseRequestCount--;
            if (coreReleaseRequestCount)
                descheduleCore();
        }
        PerfStats::threadStats.numCoreDecrements++;
    }
    PerfStats::deregisterStats(&PerfStats::threadStats);
    coreArbiter.unregisterThread();
}

/**
  * Save the current register values onto one stack and load fresh register
  * values from another stack.
  * This method does not return to its caller immediately. It returns to the
  * caller when another thread on the same kernel thread invokes this method
  * with the current value of target as the saved parameter.
  *
  * \param saved
  *     Address of the stack location to load register values from.
  * \param target
  *     Address of the stack location to save register values to.
  */
void __attribute__((noinline))
swapcontext(void **saved, void **target) {
    // This code depends on knowledge of the compiler's calling convention: rdi
    // and rsi are the first two arguments.
    // Alternative approaches tend to run into conflicts with compiler register
    // use.

    // Save the registers that the compiler expects to persist across method
    // calls and store the stack pointer's location after saving these
    // registers.
    // NB: The space used by the pushed and
    // popped registers must equal the value of SpaceForSavedRegisters, which
    // should be updated atomically with this assembly.
    asm("pushq %r12\n\t"
        "pushq %r13\n\t"
        "pushq %r14\n\t"
        "pushq %r15\n\t"
        "pushq %rbx\n\t"
        "pushq %rbp\n\t"
        "movq %rsp, (%rsi)");

    // Load the stack pointer and restore the registers
    asm("movq (%rdi), %rsp\n\t"
        "popq %rbp\n\t"
        "popq %rbx\n\t"
        "popq %r15\n\t"
        "popq %r14\n\t"
        "popq %r13\n\t"
        "popq %r12");
}

/**
  * This is the top level method executed by each thread context. It is never
  * directly invoked. Instead, the thread's context is set up to "return" to
  * this method when we context switch to it the first time.
  */
void
schedulerMainLoop() {
    while (true) {
        // Check for whether this thread should exit, for the purposes of
        // ramping down.
        if (core.threadShouldYield) {
            // Switch back to our kernel-provided stack to block in the Core
            // Arbiter, since the next time this thread unblocks, it may not
            // live on the same core, and will use a different set of user
            // contexts.
            core.threadShouldYield = false;
            swapcontext(
                    &kernelThreadStacks[core.kernelThreadId],
                    &core.loadedContext->sp);
        }
        // No thread to execute yet. This call will not return until we have
        // been assigned a new Arachne thread.
        dispatch();
        reinterpret_cast<ThreadInvocationEnabler*>(
                &core.loadedContext->threadInvocation)->runThread();
        // The thread has exited.
        // Cancel any wakeups the thread may have scheduled for itself before
        // exiting.
        core.loadedContext->wakeupTimeInCycles = UNOCCUPIED;

        // The positioning of this lock is rather subtle, and makes the
        // following three operations atomic.
        //   1. Bumping the generation number.
        //   2. Clearing the occupied bit for this context.
        //   3. Notifying joiners that this thread has fully exited.

        // It is important to notify joiners only after we have cleared our
        // occupied bit, because thread creations by the joiner will fail
        // even if this thread has logically exited. However, this context
        // cannot contend for a lock after its occupied bit has been cleared,
        // because it would never awaken once it started to spin. Thus, the
        // lock must be taken and held throughout the process of clearing the
        // occupied bit and notifying threads attempting to join this thread.
        std::lock_guard<SpinLock> joinGuard(core.loadedContext->joinLock);

        // Bump the generation number for the next newborn thread. This must be
        // done under the joinLock, since any joiner that observed the new
        // generation number might assume that the occupied bit for this
        // context is already cleared.
        core.loadedContext->generation++;

        // The code below clears the occupied flag for the current
        // ThreadContext.
        //
        // While this logically comes before dispatch(), it is here to prevent
        // it from racing against thread creations that come before the start
        // of the outer loop, since the occupied flags for such creations would
        // get wiped out by this code.
        bool success;
        do {
            MaskAndCount slotMap = *core.localOccupiedAndCount;
            MaskAndCount oldSlotMap = slotMap;
            if (slotMap.numOccupied == 0) abort();
            slotMap.numOccupied--;

            slotMap.occupied = slotMap.occupied &
                ~(1L << core.loadedContext->idInCore) & 0x00FFFFFFFFFFFFFF;
            success = core.localOccupiedAndCount->compare_exchange_strong(
                    oldSlotMap,
                    slotMap);
        } while (!success);

        // Newborn threads should not have elevated priority, even if the
        // predecessors had leftover priority
        core.privatePriorityMask &= ~(1L << (core.loadedContext->idInCore));
        *publicPriorityMasks[core.kernelThreadId] &= ~(1L <<
                (core.loadedContext->idInCore));
        PerfStats::threadStats.numThreadsFinished++;

        core.loadedContext->joinCV.notifyAll();
    }
}

/**
  * This method is used as part of cooperative multithreading to give other
  * Arachne threads on the same core a chance to run.
  * It will return when all other threads have had a chance to run.
  */
void
yield() {
    if (!core.loadedContext) return;
    if (core.localOccupiedAndCount->load().numOccupied == 1 &&
            !shutdown && !core.threadShouldYield) return;
    // This thread is still runnable since it is merely yielding.
    core.loadedContext->wakeupTimeInCycles = 0L;
    dispatch();
}

/**
  * Sleep for at least ns nanoseconds. The amount of additional delay may be
  * impacted by other threads' activities such as blocking and yielding.
  */
void
sleep(uint64_t ns) {
    core.loadedContext->wakeupTimeInCycles =
        Cycles::rdtsc() + Cycles::fromNanoseconds(ns);
    dispatch();
}

/**
  * Return a thread handle for the currently executing thread, identical to the
  * one returned by the createThread call that initially created this thread.
  *
  * When invoked from a non-Arachne thread, this function returns
  * Arachne::NullThread.
  */
ThreadId
getThreadId() {
    return core.loadedContext ? ThreadId(core.loadedContext,
            core.loadedContext->generation)
        : Arachne::NullThread;
}

/**
  * Deschedule the current thread until its wakeup time is reached (which may
  * have already happened) and find another thread to run. All direct and
  * indirect callers of this function must ensure that spurious wakeups are
  * safe.
  */
void
dispatch() {
    DispatchTimeKeeper dispatchTimeTracker;
    Core& core = Arachne::core;
    // Cache the original context so that we can survive across migrations to
    // other kernel threads, since core.loadedContext is not reloaded correctly
    // from TLS after switching back to this context.
    ThreadContext* originalContext = core.loadedContext;
    // Check the stack canary on the current context.
    if (*reinterpret_cast<uint64_t*>(core.loadedContext->stack) !=
            StackCanary) {
        ARACHNE_LOG(ERROR, "Stack overflow detected on %p. Canary = %lu."
                " Aborting...\n",
                core.loadedContext,
                *reinterpret_cast<uint64_t*>(core.loadedContext->stack));
        abort();
    }

    // Check for core release request once before checking for high priority
    // threads.
    void checkForArbiterRequest();
    checkForArbiterRequest();

    uint64_t dispatchIterationStartCycles = Cycles::rdtsc();

    // Check for high priority threads.
    if (!core.privatePriorityMask) {
        // Copy & paste from the public list.
        core.privatePriorityMask = *publicPriorityMasks[core.kernelThreadId];
        if (core.privatePriorityMask)
            *publicPriorityMasks[core.kernelThreadId] &=
                ~core.privatePriorityMask;
    }

    if (core.privatePriorityMask) {
        // This position is one-indexed with zero meaning that no bits were
        // set.
        int firstSetBit = ffsll(core.privatePriorityMask);
        if (firstSetBit) {
            firstSetBit--;
            core.privatePriorityMask &= ~(1L << (firstSetBit));

            ThreadContext* targetContext =
                core.localThreadContexts[firstSetBit];

            // Verify wakeup and occupied.
            if (targetContext->wakeupTimeInCycles == 0) {
                if (targetContext == core.loadedContext) {
                    core.loadedContext->wakeupTimeInCycles = BLOCKED;
                    DispatchTimeKeeper::numThreadsRan++;

                    // It is necessary to update core.highestOccupiedContext
                    // here because two simultaneous returns from these
                    // priority checks (the higher index blocking and the lower
                    // index exiting) can result in a gap that cannot be
                    // bridged by the core.highestOccupiedContext increment
                    // mechanism below.
                    core.highestOccupiedContext =
                        std::max(core.highestOccupiedContext,
                                core.loadedContext->idInCore);
                    return;
                }
                void** saved = &core.loadedContext->sp;
                core.loadedContext = targetContext;
                swapcontext(&core.loadedContext->sp, saved);
                originalContext->wakeupTimeInCycles = BLOCKED;
                DispatchTimeKeeper::numThreadsRan++;
                core.highestOccupiedContext =
                    std::max(core.highestOccupiedContext,
                            core.loadedContext->idInCore);
                return;
            }
        }
    }
    // Find a thread to switch to
    uint8_t currentIndex = core.nextCandidateIndex;

    for (;;currentIndex++) {

        // This block should execute when a core has iterated over exactly one
        // full cycle over the available ThreadContext's, because the value of
        // currentIndex will be saved in core.nextCandidateIndex across calls to
        // dispatch().
        while (currentIndex == core.highestOccupiedContext + 1) {
            checkForArbiterRequest();
            // Check if we need to decrement core.highestOccupiedContext
            if (core.highestOccupiedContext < maxThreadsPerCore - 1) {
                uint64_t currentWakeupCycles =
                    core.localThreadContexts[currentIndex]->wakeupTimeInCycles;
                uint64_t previousWakeupCycles =
                    core.localThreadContexts[currentIndex-1]->
                        wakeupTimeInCycles;
                if (currentWakeupCycles != UNOCCUPIED) {
                    core.highestOccupiedContext++;
                    // If we find something to run at the highest indexed
                    // context, then we should continue the current loop,
                    // skipping the code below which resets the loop state.
                    break;
                } else if (core.highestOccupiedContext > 0 &&
                        previousWakeupCycles == UNOCCUPIED) {
                    core.highestOccupiedContext--;
                }
            }

            // We have reached the end of the threads, so we should go back to
            // the beginning.
            currentIndex = 0;
            dispatchIterationStartCycles = Cycles::rdtsc();

            // Flush counters to keep times up to date
            dispatchTimeTracker.flush();

            // Check for termination
            if (shutdown)
                swapcontext(
                        &kernelThreadStacks[core.kernelThreadId],
                        &core.loadedContext->sp);

            PerfStats::threadStats.weightedLoadedCycles +=
                DispatchTimeKeeper::numThreadsRan * (
                        dispatchIterationStartCycles -
                        DispatchTimeKeeper::lastDispatchIterationStart);

            DispatchTimeKeeper::numThreadsRan = 0;
            DispatchTimeKeeper::lastDispatchIterationStart =
                dispatchIterationStartCycles;
            break;
        }

        ThreadContext* currentContext = core.localThreadContexts[currentIndex];
        if (dispatchIterationStartCycles >=
                currentContext->wakeupTimeInCycles) {
            core.nextCandidateIndex = static_cast<uint8_t>(currentIndex + 1);
            if (core.nextCandidateIndex == maxThreadsPerCore)
                core.nextCandidateIndex = 0;

            if (currentContext == core.loadedContext) {
                core.loadedContext->wakeupTimeInCycles = BLOCKED;
                DispatchTimeKeeper::numThreadsRan++;
                return;
            }
            void** saved = &core.loadedContext->sp;
            core.loadedContext = currentContext;
            swapcontext(&core.loadedContext->sp, saved);
            // After the old context is swapped out above, this line executes
            // in the new context.
            originalContext->wakeupTimeInCycles = BLOCKED;
            DispatchTimeKeeper::numThreadsRan++;
            return;
        }
    }
}

/**
  * Make the thread referred to by ThreadId runnable.
  * If one thread exits and another is created between the check and the setting
  * of the wakeup flag, this signal will result in a spurious wake-up.
  * If this method is invoked on a currently running thread, it will have the
  * effect of causing the thread to immediately unblock the next time it blocks.
  *
  * \param id
  *     The id of the thread to signal.
  */
void
signal(ThreadId id) {
    uint64_t oldWakeupTime = id.context->wakeupTimeInCycles;
    if (oldWakeupTime != UNOCCUPIED) {
        // We do the CAS in assembly because we do not want to pay for the
        // extra memory fences for ordinary stores that std::atomic adds.
        uint64_t newValue = 0L;
        __asm__ __volatile__("lock; cmpxchgq %0,%1" : "=r" (newValue), 
                "=m" (id.context->wakeupTimeInCycles),
                "=a" (oldWakeupTime) : "0" (newValue), "2" (oldWakeupTime));

        // Raise the priority of the newly awakened thread.
        if (id.context->coreId != static_cast<uint8_t>(~0))
            *publicPriorityMasks[id.context->coreId] |=
                (1L << id.context->idInCore);
    }
}

/**
  * Block the current thread until the thread identified by id finishes its
  * execution.
  *
  * \param id
  *     The id of the thread to join.
  */
void
join(ThreadId id) {
    std::unique_lock<SpinLock> joinGuard(id.context->joinLock);
    // Thread has already exited.
    if (id.generation != id.context->generation) return;
    id.context->joinCV.wait(joinGuard);
}

/**
  * This function must be called by the main application thread and will block
  * until Arachne is terminated via a call to shutDown().
  *
  * Upon termination, this function tears down all state created by init,
  * and restores the state of the system to the time before init is
  * called.
  */
void waitForTermination() {
    for (size_t i = 0; i < kernelThreads.size(); i++) {
        kernelThreads[i].join();
    }

    // We now assume that all threads are done executing.
    PerfUtils::Util::serialize();

    kernelThreads.clear();
    kernelThreadStacks.clear();
    delete[] utilizationThresholds;


    for (size_t i = 0; i < maxNumCores; i++) {
        for (int k = 0; k < maxThreadsPerCore; k++) {
            free(allThreadContexts[i][k]->stack);
            allThreadContexts[i][k]->joinLock.~SpinLock();
            allThreadContexts[i][k]->joinCV.~ConditionVariable();
            free(allThreadContexts[i][k]);
        }
        delete[] allThreadContexts[i];
        free(occupiedAndCount[i]);
        free(publicPriorityMasks[i]);
    }
    allThreadContexts.clear();
    occupiedAndCount.clear();
    publicPriorityMasks.clear();
    delete[] virtualCoreTable;
    PerfUtils::Util::serialize();

    #ifdef NO_ARBITER
    coreArbiter.reset();
    #endif
    initialized = false;
}

/**
  * This function parses out the arguments intended for the thread library from
  * a command line, and adjusts the values of argc and argv to eliminate the
  * arguments that the thread library consumed.
  */
void
parseOptions(int* argcp, const char** argv) {
    if (argcp == NULL) return;

    int argc = *argcp;

    struct OptionSpecifier {
        // The string that the user uses after `--`.
        const char* optionName;
        // The id for the option that is returned when it is recognized.
        int id;
        // Does the option take an argument?
        bool takesArgument;
    } optionSpecifiers[] = {
        {"minNumCores", 'c', true},
        {"maxNumCores", 'm', true},
        {"stackSize", 's', true}
    };
    const int UNRECOGNIZED = ~0;

    int i = 1;
    while (i < argc) {
        if (argv[i][0] != '-' || argv[i][1] != '-') {
            i++;
            continue;
        }
        const char* optionName = argv[i] + 2;
        int optionId = UNRECOGNIZED;
        const char* optionArgument = NULL;

        for (size_t k = 0;
                k < sizeof(optionSpecifiers) / sizeof(OptionSpecifier); k++) {
            const char* candidateName = optionSpecifiers[k].optionName;
            bool needsArg = optionSpecifiers[k].takesArgument;
            if (strncmp(candidateName,
                        optionName, strlen(candidateName)) == 0) {
                if (needsArg) {
                    if (i + 1 >= argc) {
                        ARACHNE_LOG(ERROR,
                                "Missing argument to option %s!\n",
                                candidateName);
                        break;
                    }
                    optionArgument = argv[i+1];
                    optionId = optionSpecifiers[k].id;
                    argc -= 2;
                    memmove(argv + i, argv + i + 2, (argc - i) * sizeof(char*));
                } else {
                    optionId = optionSpecifiers[k].id;
                    argc -= 1;
                    memmove(argv + i, argv + i + 1, (argc - i) * sizeof(char*));
                }
                break;
            }
        }
        switch (optionId) {
            case 'c':
                minNumCores = atoi(optionArgument);
                break;
            case 'm':
                maxNumCores = atoi(optionArgument);
                break;
            case 's':
                stackSize = atoi(optionArgument);
                break;
            case UNRECOGNIZED:
                i++;
        }
    }
    *argcp = argc;
}

ThreadContext::ThreadContext(uint8_t coreId, uint8_t idInCore)
    : stack(NULL)
    , sp(NULL)
    , generation(1)
    , joinLock()
    , joinCV()
    , coreId(coreId)
    , idInCore(idInCore)
    , threadInvocation()
    , wakeupTimeInCycles(threadInvocation.wakeupTimeInCycles)
{
    wakeupTimeInCycles = UNOCCUPIED;
    // Allocate memory here so we can error-check.the return value of malloc
    stack = alignedAlloc(stackSize, PAGE_SIZE);
    if (stack == NULL) {
        abort();
    }
}

/**
  * This method initialize all stacks to point at the schedulerMainLoop.
  */
void
ThreadContext::initializeStack() {
    sp = reinterpret_cast<char*>(stack) + stackSize - 2*sizeof(void*);

    // Immediately before schedulerMainLoop gains control, we want the
    // stack to look like this, so that the swapcontext call will
    // transfer control to schedulerMainLoop.
    //           +-----------------------+
    //           |                       |
    //           +-----------------------+
    //           |     Return Address    |
    //           +-----------------------+
    //     sp->  |       Registers       |
    //           +-----------------------+
    //           |                       |
    //           |                       |
    //
    // Set up the stack so that the first time we switch context to
    // this thread, we enter schedulerMainLoop.
    *reinterpret_cast<void**>(sp) = reinterpret_cast<void*>(schedulerMainLoop);

    /**
     * Decrement the stack pointer by the amount of space needed to
     * store the registers in swapcontext.
     */
    sp = reinterpret_cast<char*>(sp) - SpaceForSavedRegisters;

    /**
     * Set the stack canary value to detect stack overflows.
     */
    *reinterpret_cast<uint64_t*>(stack) = StackCanary;
}

/**
 * This function sets up state needed by the thread library, and must be
 * invoked before any other function in the thread library is invoked. It is
 * undefined behavior to invoke other Arachne functions before this one.
 *
 * Arachne will take configuration options from the command line specified by
 * argc and argv, and then update the values of argv and argc to reflect the
 * remaining arguments.
 *
 * Here are the current available options.
 *
 *     --minNumCores
 *        The minimum number of cores the application should use. This number
 *        must be at least one higher than the number of cores which are
 *        permanently exclusive.
 *     --maxNumCores
 *        The largest number of core the appliation may use
 *     --stackSize
 *        The size of each user stack.
 *
 * \param argcp
 *    The pointer to argc, the number of arguments passed to the application.
 *    This pointer will be used to update argc after Arachne has consumed its
 *    arguments.
 * \param argv
 *    The pointer to the command line argument array, which will be modiifed to
 *    remove the options that Arachne recognizes.
 */
void
init(int* argcp, const char** argv) {
    if (initialized)
        return;
    initialized = true;
    parseOptions(argcp, argv);

    if (minNumCores == 0)
        minNumCores = 1;
    if (maxNumCores == 0)
        maxNumCores = std::thread::hardware_concurrency();
    maxNumCores = std::max(minNumCores, maxNumCores);
    utilizationThresholds = new double[maxNumCores];

    std::vector<uint32_t> coreRequest({minNumCores,0,0,0,0,0,0,0});
    coreArbiter.setRequestedCores(coreRequest);
    coreReleaseRequestCount = 0;

    // We assume that maxNumCores will not be exceeded in the lifetime of this
    // application.
    virtualCoreTable = new int[maxNumCores];

    for (unsigned int i = 0; i < maxNumCores; i++) {
        occupiedAndCount.push_back(
                reinterpret_cast<std::atomic<Arachne::MaskAndCount>* >(
                    alignedAlloc(sizeof(MaskAndCount))));
        memset(occupiedAndCount.back(), 0, sizeof(std::atomic<MaskAndCount>));
        publicPriorityMasks.push_back(
                reinterpret_cast< std::atomic<uint64_t>* >(
                    alignedAlloc(sizeof(std::atomic<uint64_t>))));
        memset(publicPriorityMasks.back(), 0, sizeof(std::atomic<uint64_t>));
        // Here we will allocate all the thread contexts and stacks
        ThreadContext **contexts = new ThreadContext*[maxThreadsPerCore];
        for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
            contexts[k] = reinterpret_cast<ThreadContext*>(
                    alignedAlloc(sizeof(ThreadContext)));
            new (contexts[k]) ThreadContext(static_cast<uint8_t>(i), k);
        }
        allThreadContexts.push_back(contexts);
        virtualCoreTable[i] = i;
    }

    // Allocate space to store all the original kernel pointers
    kernelThreadStacks.resize(maxNumCores);
    shutdown = false;

    // Ensure that data structure and stack allocation completes before we
    // begin to use it in a new thread.
    PerfUtils::Util::serialize();

    // Note that the main thread is not part of the thread pool.
    for (unsigned int i = 0; i < maxNumCores; i++) {
        // These threads are started with threadMain instead of
        // schedulerMainLoop because we want schedulerMainLoop to run on a user
        // stack rather than a kernel-provided stack. This enables us to run
        // the first user thread without a context switch.
        kernelThreads.emplace_back(threadMain);
    }

    // Block until minNumCores is active, per the application's requirements.
    while (numActiveCores != minNumCores) usleep(1);
    if (!disableLoadEstimation) {
        void coreLoadEstimator();
        createThread(coreLoadEstimator);
    }
}

/**
  * This function should be invoked in unit test setup to make Arachne
  * functions callable from the unit test, which is not an Arachne thread.
  *
  * This function sets up just enough state to allow the current thread to
  * execute unit tests which call Arachne functions.
  * We assume that the unit tests are run from the main kernel thread which
  * will never swap out when running the dispatch() loop.
  */
void
testInit() {
    // NB: This technically leads to sharing memory with the highest index of
    // publicPriorityMasks, as well any other place where core.kernelThreadId is
    // used as an index.
    core.kernelThreadId = maxNumCores - 1;
    core.localOccupiedAndCount =
        reinterpret_cast<std::atomic<Arachne::MaskAndCount>* >(
            alignedAlloc(sizeof(MaskAndCount)));
    memset(core.localOccupiedAndCount, 0, sizeof(MaskAndCount));

    core.localThreadContexts = new ThreadContext*[maxThreadsPerCore];
    for (uint8_t k = 0; k < maxThreadsPerCore; k++) {
        // Technically, this allocates a bunch of user stacks which will never
        // be used, and it can be optimized out if it turns out to be too
        // expensive.
        core.localThreadContexts[k] = reinterpret_cast<ThreadContext*>(
                alignedAlloc(sizeof(ThreadContext)));
        new (core.localThreadContexts[k]) ThreadContext(~0, k);
        core.localThreadContexts[k]->wakeupTimeInCycles = BLOCKED;
        // It is important to re-initialize stacks here because some of the
        // contexts may be in the middle of dispatch calls from
        // schedulerMainLoop and switching to them will a spurious return from
        // dispatch at the start of schedulerMainLoop, which it is expensive to
        // check for.
        core.localThreadContexts[k]->initializeStack();
    }
    core.loadedContext = *core.localThreadContexts;
    *core.localOccupiedAndCount = {1, 1};
}

/**
  * This function should be invoked in unit test teardown to clean up the state
  * that makes Arachne functions callable from the unit test.
  */
void testDestroy() {
    free(core.localOccupiedAndCount);
    for (int k = 0; k < maxThreadsPerCore; k++) {
        free(core.localThreadContexts[k]->stack);
        core.localThreadContexts[k]->joinLock.~SpinLock();
        core.localThreadContexts[k]->joinCV.~ConditionVariable();

        free(core.localThreadContexts[k]);
    }
    delete[] core.localThreadContexts;
    core.loadedContext = NULL;
    *core.localOccupiedAndCount = {0, 0};
}

/**
  * This call will cause all Arachne threads to terminate, and cause
  * waitForTermination() to return.
  *
  * It is typically used only for an application's unit tests, where the global
  * teardown function in the unit test would call Arachne::shutDown() followed
  * immediately by Arachne::waitForTermination().
  *
  * This function can be called from any Arachne or non-Arachne thread.
  */
void
shutDown() {
    // Tell all the kernel threads to terminate at the first opportunity.
    shutdown = true;

    // Unblock all cores so they can shut down and be joined.
    std::vector<uint32_t> coreRequest({maxNumCores,0,0,0,0,0,0,0});
    coreArbiter.setRequestedCores(coreRequest);

}


/**
 * Attempt to acquire this resource and block if it is not available.
 */
void
SleepLock::lock() {
    std::unique_lock<SpinLock> guard(blockedThreadsLock);
    if (owner == NULL) {
        owner = core.loadedContext;
        return;
    }
    blockedThreads.push_back(getThreadId());
    guard.unlock();
    do {
        // Spurious wake-ups can happen due to signalers of past inhabitants of
        // this core.loadedContext.
        dispatch();
    } while (owner != core.loadedContext);
}

/** 
 * Attempt to acquire this resource once.
 * \return
 *    Whether or not the acquisition succeeded.
 */
bool
SleepLock::try_lock() {
    std::lock_guard<SpinLock> guard(blockedThreadsLock);
    if (owner == NULL) {
        owner = core.loadedContext;
        return true;
    }
    return false;
}

/** Release resource. */
void
SleepLock::unlock() {
    std::lock_guard<SpinLock> guard(blockedThreadsLock);
    if (blockedThreads.empty()) {
        owner = NULL;
        return;
    }
    owner = blockedThreads.front().context;
    signal(blockedThreads.front());
    blockedThreads.pop_front();
}

ConditionVariable::ConditionVariable()
    : blockedThreads() {}

ConditionVariable::~ConditionVariable() { }

/**
  * Awaken one of the threads waiting on this condition variable.
  * The caller must hold the mutex that waiting threads held when they called
  * wait().
  */
void
ConditionVariable::notifyOne() {
    if (blockedThreads.empty()) return;
    ThreadId awakenedThread = blockedThreads.front();
    blockedThreads.pop_front();
    signal(awakenedThread);
}

/**
  * Awaken all of the threads waiting on this condition variable.
  * The caller must hold the mutex that waiting threads held when they called
  * wait().
  */
void
ConditionVariable::notifyAll() {
    while (!blockedThreads.empty())
        notifyOne();
}

/**
  * Change the target of the error stream, allowing redirection to an
  * application's log.
  */
void setErrorStream(FILE* stream) {
    errorStream = stream;
}

/**
 * This function runs on a core immediately before it is deallocated, and is
 * responsible for waiting out and then migrating running threads other than
 * itself. By ensuring that the core is busy running this thread, we ensure
 * that all other threads' contexts on this core are saved.
 */
void releaseCore() {
    // Remove all other threads from this core.
    makeExclusiveOnCore(true);
    core.threadShouldYield = true;
}

/**
  * This function can be called from any thread to increase the number of cores
  * used by Arachne.
  */
void incrementCoreCount() {
    std::lock_guard<SpinLock> _(coreChangeMutex);
    if (coreChangeActive) return;
    if (numActiveCores >= maxNumCores) return;

    coreChangeActive = true;
    ARACHNE_LOG(NOTICE, "Attempting to increase number of cores %u --> %u\n",
            numActiveCores.load(), numActiveCores + 1);
    #if TIME_TRACE
    TimeTrace::record("Start Core Count %d --> %d",
            numActiveCores.load(), numActiveCores + 1);
    #endif
    std::vector<uint32_t> coreRequest({numActiveCores + 1,0,0,0,0,0,0,0});
    coreArbiter.setRequestedCores(coreRequest);
}

/**
  * This function can be called from any thread to attempt to decrease the
  * number of cores used by Arachne. It may return before a core is actually
  * released, and there is no guarantee that a core will be released.
  */
void decrementCoreCount() {
    std::lock_guard<SpinLock> _(coreChangeMutex);
    if (coreChangeActive) return;
    if (numActiveCores <= minNumCores) return;

    coreChangeActive = true;
    ARACHNE_LOG(NOTICE, "Attempting to decrease number of cores %u --> %u\n",
            numActiveCores.load(), numActiveCores - 1);
    #if TIME_TRACE
    TimeTrace::record("Start Core Count %d --> %d",
            numActiveCores.load(), numActiveCores - 1);
    #endif

    std::vector<uint32_t> coreRequest({numActiveCores - 1,0,0,0,0,0,0,0});
    coreArbiter.setRequestedCores(coreRequest);
}

/*
 * If the Core Arbiter asks the Arachne runtime to yield a core, this function
 * shall begin the process of descheduling a core.
 *
 * The caller must hold the coreChangeMutex when making this call.
 */
void descheduleCore() {
    // Find a core to deschedule
    uint8_t minLoaded =
        occupiedAndCount[virtualCoreTable[0]]->load().numOccupied;
    int minIndex = 0;
    for (uint32_t i = 1; i < numActiveCores; i++) {
        uint32_t coreId = virtualCoreTable[i];
        if (occupiedAndCount[coreId]->load().numOccupied < minLoaded) {
            minLoaded = occupiedAndCount[coreId]->load().numOccupied;
            minIndex = i;
        }
    }

    // Give up if the minLoaded core is exclusive or full, since that implies
    // we are likely pre-empting must-have cores.
    if (minLoaded >= static_cast<uint8_t>(56)) {
        coreChangeActive = false;
        ARACHNE_LOG(DEBUG, "Failed to find an unoccupied core, giving up!\n");
        ARACHNE_LOG(DEBUG, "minLoaded = %u, minIndex = %u!\n",
                minLoaded, minIndex);
        for (uint32_t i = 0; i < numActiveCores; i++) {
            uint32_t coreId = virtualCoreTable[i];
            ARACHNE_LOG(DEBUG, "virtualCoreId = %d, coreId = %u,"
                    " numOccupied = %d, occupied = %lu\n",
                   i, coreId, occupiedAndCount[coreId]->load().numOccupied,
                    occupiedAndCount[coreId]->load().occupied);
        }
        return;
    }

    // Create a thread on the target core to handle the actual core release,
    // since we are currently borrowing an arbitrary context and should not
    // hold it for too long.
    // If this creation fails, it would implies that we are overloaded and
    // should not ramp down.
    if (createThreadOnCore(minIndex, releaseCore) == NullThread) {
        coreChangeActive = false;
        ARACHNE_LOG(WARNING, "Release core thread creation failed to %d!\n",
                minIndex);
    }
}

/**
  * Detect requests for cores from the core arbiter.
  */
void checkForArbiterRequest() {
    if (!coreArbiter.mustReleaseCore())
        return;
    std::lock_guard<SpinLock> _(coreChangeMutex);
    coreReleaseRequestCount++;

    if (coreReleaseRequestCount >= numActiveCores)
        abort();

    // Deschedule a core iff we are the first thread to read that a
    // core release is needed.
    if (coreReleaseRequestCount == 1)
        descheduleCore();
}

/**
  * This function is invoked from an Arachne thread and does not return until
  * it is the only thread on the core. If it returns true, it is valid for this
  * thread to never yield, because no other threads will share its core.
  *
  * Note that if too many threads call this function, Arachne will not be able
  * to offer very much concurrency.
  *
  * NB: If a thread invokes this function, it must invoke makeSharedOnCore
  * before exiting. We require this because this call is expected to be used
  * infrequently, and it does not make sense to pay the performance penalty for
  * checking on every thread exit. A caveat of this requirement is that a
  * thread which never exits until program termination has no need to invoke
  * makeSharedOnCore.
  */
bool makeExclusiveOnCore(bool forScaleDown) {
    // Cache the original context so that we can survive across migrations to
    // other kernel threads, since core.loadedContext is not reloaded correctly
    // from TLS after switching back to this context.
    ThreadContext* originalContext = core.loadedContext;

    std::lock_guard<SleepLock> _(coreExclusionMutex);
    // Already exclusive
    // TODO(hq6): Read the new value of core.localOccupiedAndCount, instead of
    // the value from the previous kernelThread.
    MaskAndCount originalMask = *core.localOccupiedAndCount;
    if (originalMask.numOccupied > maxThreadsPerCore) {
        // If we are not the exclusive thread, then an error has occurred.
        if (originalMask.occupied ==
                (1U << originalContext->idInCore)) {
            return true;
        } else {
            abort();
        }
    }

    // Block future creations on core
    MaskAndCount targetOccupiedAndCount;
    MaskAndCount blockedOccupiedAndCount;
    bool success = false;
    do {
        targetOccupiedAndCount = *core.localOccupiedAndCount;
        blockedOccupiedAndCount = targetOccupiedAndCount;
        blockedOccupiedAndCount.numOccupied = EXCLUSIVE;
        success = core.localOccupiedAndCount->compare_exchange_strong(
                targetOccupiedAndCount, blockedOccupiedAndCount);
    } while (!success);


    // Wait out creations that finished CASing before we blocked creations
    bool pendingCreation;
    do {
        pendingCreation = false;
        for (int i = 0; i < maxThreadsPerCore; i++) {
            // It is safe to use targetOccupiedAndCount here, because the
            // success of the CAS indicates that all subsequent attempts to CAS
            // should have failed.
            // There is no race with completions here because no other thread
            // can be running on this core since we are running.
            if (((targetOccupiedAndCount.occupied >> i) & 1) &&
                    core.localThreadContexts[i]->wakeupTimeInCycles ==
                    UNOCCUPIED) {
                pendingCreation = true;
                break;
            }
        }
    } while (pendingCreation);

    // Wait out thread completions
    // If this interval is discovered to be too long, we can take a sleep &
    // poll approach.
    sleep(COMPLETION_WAIT_TIME);

    blockedOccupiedAndCount = *core.localOccupiedAndCount;
    // Sanity checking that we blocked creations successfully
    if (blockedOccupiedAndCount.numOccupied <= maxThreadsPerCore) {
        abort();
    }


    // Remap this slot in virtualCoreTable to point at the highest number, to
    // make it easier to pawn off work.
    for (uint32_t i = 0; i < maxNumCores; i++)
        if (virtualCoreTable[i] == core.kernelThreadId) {
            virtualCoreTable[i] = virtualCoreTable[numActiveCores - 1];
            virtualCoreTable[numActiveCores - 1] = core.kernelThreadId;
            break;
        }

    // Migrate off all threads other than the current one.  Round robin among
    // cores because these are likely long-running threads.
    int nextMigrationTarget = 0;
    int coreId = virtualCoreTable[nextMigrationTarget];


    for (uint8_t i = 0; i < maxThreadsPerCore; i++) {
        if (i == originalContext->idInCore) {
            // Skip over ourselves
            continue;
        }
        if ((blockedOccupiedAndCount.occupied >> i) & 1) {
            uint8_t index;
            do {
                // Each iteration through this loop makes one attempt to
                // enqueue the task to the specified core. Multiple iterations
                // are required only if there is contention for the core's
                // state variables.
                MaskAndCount slotMap = *occupiedAndCount[coreId];
                MaskAndCount oldSlotMap = slotMap;

                // Skip this core since it might be an exclusive or fully
                // loaded.
                if (slotMap.numOccupied >= maxThreadsPerCore) {
                    success = false;
                    break;
                }

                // Search for a non-occupied slot and attempt to reserve the
                // slot
                index = 0;
                while ((slotMap.occupied & (1L << index)) &&
                        index < maxThreadsPerCore)
                    index++;

                slotMap.occupied =
                    (slotMap.occupied | (1L << index)) & 0x00FFFFFFFFFFFFFF;
                slotMap.numOccupied++;
                success = occupiedAndCount[coreId]->compare_exchange_strong(
                            oldSlotMap, slotMap);
            } while (!success);

            if (success) {
                // Now that we have found a slot, we can clear our bit.
                blockedOccupiedAndCount.occupied &=
                    ~(1 << i) & 0x00FFFFFFFFFFFFFF;
                // At this point we've reserved a spot on the target, and now
                // we swap.
                ThreadContext* contextToMigrate =
                    allThreadContexts[coreId][index];
                allThreadContexts[coreId][index] = core.localThreadContexts[i];
                core.localThreadContexts[i] = contextToMigrate;

                // Update idInCore to a consistent value
                allThreadContexts[coreId][index]->idInCore = index;
                core.localThreadContexts[i]->idInCore = i;

                allThreadContexts[coreId][index]->coreId =
                    static_cast<uint8_t>(coreId);
                core.localThreadContexts[i]->coreId =
                    static_cast<uint8_t>(core.kernelThreadId);
                // Only increment if we succeeded.
            } else {
                // Try again if we failed to find a slot to pawn our work onto.
                i--;
            }

            // The next victim core that we will pawn our work on.
            nextMigrationTarget =
                (nextMigrationTarget + 1) % (numActiveCores - 1);
            coreId = virtualCoreTable[nextMigrationTarget];
        }
    }

    // Sanity checking that we are the only thread left on this core.
    int count = 0;
    for (int i = 0; i < maxThreadsPerCore; i++)
        if (blockedOccupiedAndCount.occupied & (1L << i))
            count++;
    if (count != 1) {
        abort();
    }

    // Update core.localOccupiedAndCount to a consistent state before exiting.
    // At this point, creations should have already been blocked, and
    // completions cannot occur because we are running, so we can just directly
    // assign.
    *core.localOccupiedAndCount = blockedOccupiedAndCount;

    if (!forScaleDown) {
        numExclusiveCores++;
    }

    return true;
}

/**
  * This function reverses the effect of makeExclusiveOnCore(), allowing other
  * threads to once again be scheduled onto the core hosting this thread.
  *
  * If makeExclusiveOnCore has never been invoked from the current thread, then
  * this function is a no-op.
  */
void makeSharedOnCore() {
    std::lock_guard<SleepLock> _(coreExclusionMutex);
    // If not exclusive, this is a no-op.
    if (core.localOccupiedAndCount->load().numOccupied < maxThreadsPerCore)
        return;
    // Assume already exclusive
    MaskAndCount original = *core.localOccupiedAndCount;
    MaskAndCount shared = original;
    shared.numOccupied = 1;
    bool success = core.localOccupiedAndCount->compare_exchange_strong(
            original, shared);
    if (!success) {
        // If this scenario happens, it means there is a bug since nobody
        // should be able to create threads on an exclusive core.
        ARACHNE_LOG(ERROR, "Error making core shared again! Aborting...\n");
        abort();
    }
    numExclusiveCores--;

    // The time that this core spent in exclusive mode should not be counted
    // towards utilization of shared cores.
    DispatchTimeKeeper::lastTotalCollectionTime = 0;
}

/**
  * Periodically wake up and observe the current load in Arachne to determine
  * whether it is necessary to increase or reduce the number of cores used by
  * Arachne.
  */
void coreLoadEstimator() {
    PerfStats previousStats;
    PerfStats::collectStats(&previousStats);


    for (PerfStats currentStats; ; previousStats = currentStats) {
        sleep(MEASUREMENT_PERIOD);
        PerfStats::collectStats(&currentStats);

        // Take a snapshot of currently active cores before performing
        // estimation to avoid races between estimation and the fulfillment of
        // a previous core request.
        uint32_t curActiveCores = numActiveCores;

        // Exclusive cores should contribute nothing to statistics relevant to
        // core estimation during the period over which they are exclusive.
        int numSharedCores = curActiveCores - numExclusiveCores;

        // Evalute idle time precentage multiplied by number of cores to
        // determine whether we need to decrease the number of cores.
        uint64_t idleCycles =
            currentStats.idleCycles - previousStats.idleCycles;
        uint64_t totalCycles =
            currentStats.totalCycles - previousStats.totalCycles;
        uint64_t utilizedCycles = totalCycles - idleCycles;
        uint64_t totalMeasurementCycles =
            Cycles::fromNanoseconds(MEASUREMENT_PERIOD);
        double totalUtilizedCores =
            static_cast<double>(utilizedCycles) /
            static_cast<double>(totalMeasurementCycles);

        // Estimate load to determine whether we need to increment the number
        // of cores.
        uint64_t weightedLoadedCycles =
            currentStats.weightedLoadedCycles -
            previousStats.weightedLoadedCycles;
        double averageLoadFactor =
            static_cast<double>(weightedLoadedCycles) /
            static_cast<double>(totalCycles);
        if (curActiveCores < maxNumCores &&
                averageLoadFactor > loadFactorThreshold) {
            // Record our current totalUtilizedCores, so we will only ramp down
            // if utilization would drop below this level.
            utilizationThresholds[numSharedCores] = totalUtilizedCores;
            incrementCoreCount();
            continue;
        }

        // We should not ramp down if we have high occupancy of slots.
        double averageNumSlotsUsed = static_cast<double>(
                currentStats.numThreadsCreated -
                currentStats.numThreadsFinished) /
                numSharedCores / maxThreadsPerCore;

        // Scale down if the idle time after scale down is greater than the
        // time at which we scaled up, plus a hysteresis threshold.
        if (totalUtilizedCores < utilizationThresholds[numSharedCores - 1]
                - idleCoreFractionHysteresis &&
                averageNumSlotsUsed < SLOT_OCCUPANCY_THRESHOLD) {
            decrementCoreCount();
        }
    }
}

} // namespace Arachne
