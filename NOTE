Functions to be implemented
0. thread_lib_init()
    a. Allocate_stacks (separate stack pools for each core)
    b. Create kernel-level threads (or processes) which are pinned to particular cores.
2. create_thread_and_enqueue(function) // No arguments, just pass a lambda using new C++ features. Assume on same core.
3. schedule() (ask the scheduler to invoke a function on a new user-level stack)
4. yield() - return control to the scheduler (need to switch stacks)

Use CPP for lambdas

Eventually, need to run experiments where we deal with blocking IO, and non-blocking IO for RAMCloud.
 - Handling system calls and whatnot.
 - Get smart about core-aware scheduling, so that user-level threads respect _cores_ and not kernel threads.


Perhaps the kernel threads should allocate their own stack *pools* in a thread-local fashion?


John means you need to seed all the work before joining the thread pool,
otherwise you'll have everyone in the thread pool polling for work and nobody
to put work on.
 - The other solution is to 
 - The related question is - how does the application terminate? Someone has to
   call terminate.

Also, where is application code and where is the library code?
 - We can ask the application to call join_thread_pool after it has finished seeding the work, but that is a little annoying.

The worker queues should be protected by a SpinLock, a la RAMCloud.
  - The key idea is that these normal pool threads should NEVER trap to the
    kernel, because if they do then we've lost a core for the duration of that time.

Write the guts and then pull SpinLock out and start debugging.

You have to handle the case where the thread terminates as well as yielding.
 - They probably share a library function

Think about what happens on an explicit yield

Where is the CS 110 project?
    https://mail.google.com/mail/u/0/#search/cps+110+thread/11f811eeb78cacfc

    - That API basically required that application was structured in a way that the thread library initialization was the very first function

It is a well-defined point for each kernel thread. The same EIP, but a different ESP.

In the current system, there is only a single per-core run queue. Nothing else. Just a dispatcher.
 - Just need to be able to yield and return.
 - Start without yield, just schedule and return

Based on the following code, it seems that the stack needs to be set up in a certain way
    http://www.opensource.apple.com/source/Libc/Libc-825.25/i386/gen/makecontext.c

Let's use setjmp for now. It will faster than writing assembler.
    http://stackoverflow.com/questions/7969075/about-setjmp-longjmp

We need to get a baseline on this ping before we can know if any asm we write
will improve things.
 - So starting with the setjmp is a good sanity check even if it does save more
   registers than it needs, although the compiler should know which registers
   it needs to save.

May also want to disable interrupts when library code is running to avoid multi-swap.
 - Although it's not clear if it matters when the thread is in user-land

Also no unit tests yet, still getting basics up and running.

Names are nice to have for debugging, but not yet needed for this.

http://stackoverflow.com/questions/2560792/multitasking-using-setjmp-longjmp
 - Screw portability to non-POSIX systems?
 - Changing the stack and the EIP also

One more thing -- we need to figure out how to reclaim stacks after a user-level thread finishes. That's also not yet implemented.
 - Maybe the point where we longjmp to should also handle this part
 - Either that or the stacks can be a circular buffer where we track the
   beginning and ending node.
 - that means when we are taking something off the run queue, we need to
   determine whether it needs a new stack or not.

https://gcc.gnu.org/onlinedocs/gcc-4.6.2/gcc/Return-Address.html#Return-Address
 - Non-portable return address

 Remember to initialize your stack pointer to NULL when you first create the WorkUnit.
    - Or just assign it in the function add_task();
  - Perhaps a constructor would be in order
Need to implement thread_yield() also as well as add_task().

Since only one (real) kernel thread can be executing at a time, and only that thread touches the stack queue, we should not require a lock to protect the stack queue.

Fix compiler errors, and then perhaps fall back to setcontext / makecontext until I'm ready to write assembler.

http://www.1024cores.net/home/lock-free-algorithms/tricks/fibers
 - use this trick and try it and see how well it works.
 - Or maybe makecontext is fine too.

Remember, we need threadWrapper so that library code can run on the user thread's stack after the user thread exits, and the user thread won't just fall off a cliff.

You need a baseline before you can play the assembler trick. Start from a point of known correctness.

One workaround is to make threadInit never return (ie take in a function to become), but that would go against co-existence, or would it?
    - One thread turns into the threadInit function
    - For now, we can have a manual separation (init, add initial task, then join the
      threadpool)


http://www.linuxplumbersconf.org/2013/ocw/system/presentations/1653/original/LPC%20-%20User%20Threading.pdf

Without the context change, you don't manage the stack, so all you have a set
of short-lived events.
 - let's build this and run an application on it before we do the other thing.

don't forget to enable optimizations

The WorkUnit structure is purely internal so it may not belong in a public header.

Also for thread pinning and stuff, perhaps I should take a dependency on the
PerfUtils which I had already factored out earlier.

The Main thread can join the pool without needing a thread instance.
 - The other data structures can be used directly
 Library should probably return the most unloaded core.


Could I be pointing at the wrong end of the stack?

Also, we are failing after the double-yield, so look at that path.
Mayhaps we need logging also?

Invariant: A completed running thread should never become re-enqueued

Remember to assume nothing. Just because it appears to work correctly does not
mean it is correct.

We shall see if asm is necessary. We also need to pin kernel threads lols...


--------------------------------------------------------------------------------
The current status seems to be that the stack pointer got corrupted somehow.

(gdb) bt
#0  0x00007ffff7313cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff73170d8 in __GI_abort () at abort.c:89
#2  0x00007ffff7350394 in __libc_message (do_abort=do_abort@entry=2, fmt=fmt@entry=0x7ffff745c52b "*** %s ***: %s terminated\n") at ../sysdeps/posix/libc_fatal.c:175
#3  0x00007ffff73e7c9c in __GI___fortify_fail (msg=<optimized out>) at fortify_fail.c:37
#4  0x00007ffff73e7bad in ____longjmp_chk () at ../sysdeps/unix/sysv/linux/x86_64/____longjmp_chk.S:100
#5  0x00007ffff73e7b09 in __longjmp_chk (env=0x1, env@entry=<error reading variable: Asked for position 0 of stack, stack only has 0 elements on it.>, val=val@entry=0)
    at ../setjmp/longjmp.c:38
    #6  0x000000000040283c in Arachne::threadMainFunction (id=3) at Arachne.cc:154
    #7  0x00000000004029a2 in Arachne::mainThreadJoinPool () at Arachne.cc:211
    #8  0x00000000004012cf in main () at ArachneTest.cc:35

Remember, you need to save the RETURN Address, because that is precisely where
you want to jump immediately after you swap back in. That way, you return to
the instruction that would have run immediately AFTER you regain the context.

Savecontext:
 save eip
 push callee-saved registers
 save ebp # just saving a register 
 save esp

loadcontext
 - load esp
 - load ebp
 - pop callee-saved registers


swapcontext

 - Figure out how to link in ASM - should be no optimization effects there.
    - Write that, test it, measure it, and then figure out whether we can do better by saving fewer registers into our structure.
Will the stack layout be the same with and without optimizations?

http://www.ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html#s2
Crap, how do I pass arguments?

Using extended asm might be the best shot.

Only care about arguments on the first call.

Why are we passing the address of running? Because that is the true function that we are calling?
- but we don't really need it, do we?
Need to figure out how to arrange the stack and registers to pass an argument.

Could swapcontext be sufficient? 
 - Is there actually a need for save and set context?
 - would it simplify things if we didn't need to always swapcontext?
Coming back, it could just go through the loop again, which would indicate that there is no need for saving and setting context.
Let's write all three, and then reduce it if necessary.
How do you set the register values for the initial context?

How do you differentiate between setcontext with arguments vs without arguments?
 how does pintos do that?
 How does pintos handle this?

It looks like pintos just lays the argument on the stack frame so that the function when invoked
John said that you "return" to a particular place, rather than jump to it.

How can I get the same mechanism for passing arguments and not passing arguments?
 - How can we have a unified function?
 - How does Golang handle argument passing?
 - Is it just hardcoded?


Also, how do you ensure that the first time you switch you have the arguments and the second time you switch, you do not have arguments?
 - The return address is at a different location.


On the golang side, the go functions do take an argument. So how are those passed in the initial call?
Here's how the stack is aligned:
     sp := newg.stack.hi - totalSize
     spArg := sp
      - the high end of the stack.
      - newg.stack is where it actually is
    runqput(_p_, newg, true)
     - So what happens to the stack when the function unwinds?

The argument is put at the top of the stack, effectively...
 - Actually, we might be okay because the first invocation will have an IP at the function, which will try to pop off the arguments from the stack (those will be arranged onto the stack (or the registers) by the initial setup function)
    - The second invocation will have an IP that doesn't adjust the stack.
    - Will that simply work?
Go just puts it onto the stack, which is fine, but I have to follow the standard convention for passing arguments.
 - That is precisely why I need setcontext.
 - But then how do I return to library using the same function?

Swapcontext can always be a "return" but setcontext may need to be a jmp instruction as go does it.

https://en.wikipedia.org/wiki/X86_calling_conventions#x86-64_calling_conventions
 - It seems that the standard calling convention is that arguments are passed
   through registers. Unfortunately, this may not work because I cannot prevent
   gcc from changing registers when I make my function call.
 - Thus, I will need to create my own calling convention and pull the argument
   off the stack instead of registers, unless I want to jump directly instead
   of using a call. 
    - Or do I?

I need to understand the difference between CALL and JMP.
    http://stackoverflow.com/questions/32793117/assembly-call-vs-jmp
    - It looks like I can just push my argument onto the stack and do a manual
      jmp, assuming that the function will never return (it will instead jump
      back to the library context using my code).
 - But local variables may also be allocated onto the stack, so how do I handle that?

The simplicity in Go is that they control the compiler, so they know what the function they are jumping to will look like.


Perhaps thread_wrapper should be an asm routine that will receive the argument on the stack and then call the real function?

https://www.cs.uaf.edu/2012/fall/cs301/lecture/09_24_call_and_ret.html

In Capricio, they also have a thread wrapper to clean up when a thread's function finishes. - This seems to be a required component of all such systems.

https://github.com/bernied/capriccio/blob/master/src/threads/threadlib.c

      t->initial_func = func;
      t->initial_arg = arg;
      - How are these things used in the jump?

The new_thread_wrapper directly calls current_thread function with the current htread argument, but my question is: How does it get the current thread?
    - Or effectively, if current_thread is thread_local (ie Global to the current kernel thread), it may not need to pass an argument at all.
 - However that function does at least appear to take an argument, which I find
   a little strange, since it doesn't use that. Better check how that initial context switch works from the run queue.


It appears to co_call leads to a pseudocode - where is the real asm exactly?
    /home/hq6/Code/src_for_oss_projects/capriccio/src/threads/threadlib.c
    - For various platforms, they never show the actual ASM code to do the jump?
    - Need to find the original source.

However, it does not appear that any real arguments are passed, so it seems strange that their thread wrapper function takes an argument. It seems that this may be a fake argument that allows the jump into the function to work more effectively.
 - Ah grepping for that function does reveal some things
     vi threads/coro-1.1.0.pre2/arch/x86-linux/coro.c

 It looks like co_call is supposed to handle passing an argument to the
 function, and the thread wrapper was intended to receive the argument, but in
 practice only NULL is ever passed. Perhaps they tried and they failed?
 - It is not really necessary to pass that argument though, as long as we always set running immediately before jumping to that function.

Find the source code for swapcontext / setcontext in Linux. How exactly do they pass an argument in x86_64?

--------------------------------------------------------------------------------
Wed 06 Jan 2016 10:37:42 PM PST
 - Recalling that John asked me to refactor the code into C++ style, which I can do after I get the dispatcher working.
 - I may have mentioned at the last status update that I'd started that process.


http://eli.thegreenplace.net/2011/09/06/stack-frame-layout-on-x86-64/
http://zenit.senecac.on.ca/wiki/index.php/X86_64_Register_and_Instruction_Quick_Start
https://gcc.gnu.org/onlinedocs/gcc/Basic-Asm.html
https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html#Clobbers

Use the simplest thing that works. Do not use any extra features if you do not need them.

Where are the notes from the last discussion with John?
 - The discussion about whether we should just set up the library so that we control "main" and instead give some other main to the user program running under Arachne.
 - I seem to have misplaced that particular set of notes somewhere.
 - In the future may be best to use a single file and just use the timestamp tag...

http://stackoverflow.com/questions/5485468/x86-assembly-pushl-popl-dont-work-with-error-suffix-or-operands-invalid
 - Note that I'm using rax as scratch, but that may not be valid if rax is one of the callee-saved registers (should double-check that)

--------------------------------------------------------------------------------
Sun 10 Jan 2016 11:31:55 AM PST
 - Crap. I don't think I'm debugging what I think I'm debugging if I cannot even find the actual library functions that are invoked by my client application...
 
 - Oh, I see, gdb is confused by the asm. Need to si and disas
 - first break  Arachne.cc:135
--------------------------------------------------------------------------------
Mon 11 Jan 2016 04:50:26 PM PST
    x/20i $pc
     - Cannot use disas because it tries to find function.

--------------------------------------------------------------------------------
Wed 13 Jan 2016 03:06:14 PM PST
   si 7 

It appears that swapcontext has failed.
 - What do I expect to happen?
 - That is the ultimate question
 - I expect the PC to change on the return into the new thread's code.

Where is the stack and where is the return address relative to the stack?
Crap, it looks like swapcontext got inlined....
 - Arguments did not get passed as expected.
 - Fully assembly function in pintos seems to actually receive arguments.

http://www.eecs.umich.edu/courses/eecs373/readings/Assembler.pdf

Make a fake stack frame and use return
 - High level bit is that I don't completely understand what return is doing yet.

TODO: Watch a normal function call without optimizations and see how that one works

mov src, dst in the AT&T syntax order

    http://eli.thegreenplace.net/2012/01/03/understanding-the-x64-code-models

I can do a jump, so the next steps are as follows
    1) Save and try to jump back to the library context with setContext. DONE.
    2) Figure out how to set up the stack correctly (make sure the pointer points at the correct end of the stack)
        - In particular, the argument must get passed correctly to the target function.
        - To get the argument off correctly, we have to know (a) where our C
          variable is stored and (b) where our input is stored on the stack
          relative to the pointer.
    3) Figure out how to use ret correctly (maybe needs a leave first) // This is currently still open and not really working.
    4) Save all the callee saved registers (unless the preamble to the function already does it)


It appears that Pintos has a special struct that literally maps onto the stack
frame, including function arguments, which makes things super convenient for the wrapper
function, since it thinks it is a normal function with an argument.

Can I do the same?
I see no reason why not, as long as I know the correct stack layout.
 - The only gotcha is that arguments are passed in registers in x64 land,
   whereas the pintos calling convention has all arguments passed on the stack.
 - A POD struct in C++ should be equivalent to a POD struct in C.

Why can't I just use the `running` variable? It is TLS, no?
 - I wanted to learn how to pass the argument, which is why I started doing it this way.
 - But yes, in theory, I could just use running directly and avoid this whole mess of argument passing with registers.
 - However, this is also a good sanity check for my stack layout understanding.
 - Also, I could have made the thread start function itself a closure, thus avoiding the argument passing issue entirely

Observations
 - Now the return address in the thread library appears to be incorrect.
 - It seems to be pointing back into the new thread, not the old thread. We
   shall see which offset the correct address is stored at, and use that offset.
 - The instruction pointer is stored on the top of the old stack which we kept around, so there may not be a need to store it separately.
 - We should optimize this out for simplicity of code. This has been done.
 - Now we shall try to execute the actual work function, which will of course segfault.
 - It looks like callq does not do anything like what I think it does. It is very different than a jmp.
    - Near calls and far calls galore
 - It seems that the page range containing the function code does not have exec privileges, which may be why it is segfaulting.
    - How did that happen?
 - Compare a normal invocation vs my invocation.
 - It appears that gcc is actually taking steps to protect the register $rdi
   from my assembly instruction, by backing it up in the register rbx before my
   code gets a chance to run.
 - TLS may still be expensive, so perhaps I should implement that myself to make the dispatching faster.


Crap, two user threads cause the program to hang. :(
 - More bugs guaranteed
 - Although this might just be a matter of not saving enough state.
ret just ...didn't work.
 - Now there might be a fight over `running`

Thoughts on Return vs Jmp
 - jmp gives you more fine-grained control in some sense, but it doesn't
   necessarily make sense to use the value of esp from the previous call.
 - However, ret lets you simple mimic the stack layout of the real function
   call, and return exactly like a real function call without worrying as much
   about the details such as exact esp value.


Yield issues
 - Is there any reason why yield would return to library instead of directly
 swapping to the next thread? Perhaps later the library will want to implement
 priorities and want to run that code on its own stack instead of a host
 thread's stack.
  - When we try to switch to the second thread, running.context is screwed up
 - It appears that the stack is getting screwed up by the jumps. That may be
what is causing incorrect arguments to be passed to the subsequent call to
swapcontext.

--------------------------------------------------------------------------------
Thu 14 Jan 2016 02:26:01 PM PST

Debugging with John (clarify of vision)
 - John suggested changing the interface to just a stack pointer, since everything can in theory be stored on the stack during a context switch.
 - Also, when he looked at Intel documentation, he seems to be able to click on the left at the index for instructions
 - I need to use xpdf to view the thing, not evince. Then I get a table of contents.
    http://askubuntu.com/questions/18495/which-pdf-viewer-would-you-recommend

 - Switched to using only ret, and now getting a strange thread error which is
   most likely the result of memory corruption since I do link in the
   -lpthreads
 - Do I actually need a data structure to hold the thread objects, or can I
 simply detach them since I already have their queues?

What happens when you yield and then fall off a cliff when you return from the yield?

Expected Behavior:
 - Yield should simply return, and eventually printEveryTwo would finish, which
   would take us back to the libraryContext.

Actual Behavior:
 - We get the following error
     terminate called after throwing an instance of 'std::system_error'
       what():  Enable multithreading to use std::thread: Operation not permitted
       http://stackoverflow.com/questions/17274032/c-threads-stdsystem-error-operation-not-permitted
    - Still a segfault after two threads are up though

 - John's idea of pooling them may make sense.
 - Based on Pintos and The CS 110 project, it seems that there's no real reason
   for the scheduler code to run in the library context. It can run directly on the call to yield.

    - Yield is completely broken / incorrect right now. Need to fix it.
    - At the very least you want it to be the case that UserContext is a
    pointer or a **, because then when you store something the copy, it still
    propogates to the real context / stack pointer.
    - Need to watch for memory leaks if you are using a pointer, and make sure you recycle the WorkUnit memory.
    - Maybe start out with a pool of these things and maintain the high water mark.
Can return but swapcontext is working.
--------------------------------------------------------------------------------

Fri 15 Jan 2016 11:11:17 AM PST

Two problems at the moment
 1) Somehow there is non-null stack on the other thread and I do not understand why yet.
 2) Threads without stacks cannot be run directly without initializing their stack.

There are a couple of places where the code falls off a cliff
 - If we try a yield and we cannot run the next thread beceause of lack of stack resources, then we fall off a cliff.
 - It looks like the stack is broken, because we called setcontext instead of swapcontext.
 - We need to think carefully about setcontext vs swapcontext
    !! Decompose the problem of stack assignment and setup from the problem of switching context. 
    - Need to reorganize the code to reflect this separation.
    - We still do need to distinguish between threads with stacks and threads without and behave differently, but need to be very careful. 

--------------------------------------------------------------------------------
Tue 19 Jan 2016 10:51:48 PM PST

Current Items
 >> Find and fix all current bugs that prevent ArachneTestApp from working. DONE.
 >> Fix swapcontext/setcontext to save the proper callee-saved registers on the stack based on the x64 calling convention. DONE.
   - Remember to fix the way you set up the stack in the initialization as well.
   && http://stackoverflow.com/questions/18024672/what-registers-are-preserved-through-a-linux-x86-64-function-call
   && http://stackoverflow.com/questions/7902903/gcc-x64-function-calling
 >> Fix ASM to avoid the possibility of gcc reordering
   && https://gcc.gnu.org/onlinedocs/gcc/Basic-Asm.html#Basic-Asm
 >> Change the API for swapcontext to be a single stack pointer only.
 >> Set up the Git remotes
 >> Run the benchmark that John asked for to see the ping.
    - Currently we see 500 ns even without saving all the proper registers. // My code is too slow; uses STL.
        - Or is this because I'm on my laptop with power saving?
        - Or perhaps this is the lock overhead?
    - Is this the TLS overhead we are seeing?
    - Run the same benchmark on rc01 to see the difference.
        - Still 200 ns
        - Time to insert timetraces and use ttsum
 - Identify the bottlenecks and eliminate them
    - Add Time Tracing code
    - TLS? STL? Dynamic memory allocation?
 - Refactor code to be C++ style
 - Get rid of the silly passing of the running argument.
 - Decide whether you need savecontext; it doesn't seem like I do right now
 !! After identifying the bottleneck on the STL, ask him whether I should write my own simple data structures or look for faster libraries.
    - Most likely the answer will be "write your own"
    - Use intrusive lists?
    - Avoid dependencies on external?
--------------------------------------------------------------------------------
Wed 20 Jan 2016 02:56:38 PM PST

Current Items
 - Identify the bottlenecks and eliminate them
>> It seems that TimeTrace overhead is much higher than it is in RAMCloud.
   300 ns total overhead when we reduce the number of kernel threads.
   - Without the traces, it only costs about 70-80ns, but we should run aggregates on that.
 - Debugging strange segfault with FPU
 - It looks like gdb uses AT&T syntax by default, which is src,dst instead of dst,src
    - That would imply the current segfault is trying to write to a memory location that does not exist
   && http://stackoverflow.com/questions/972602/att-vs-intel-syntax-and-limitations 
 - This doesn't happen when I print to stdout, but it does happen when I use TimeTrace.
 - One printf is enough to screw it up
>> Remove the cute argument passing on the stack and switch to just using the (kernel) thread local state
    - That appears to have fixed it, although it is not clear why.
 - Two possible ideas for dealign with register initialization
    1) Initialize them to the same values they were in for the parent thread.
    2) Observe a direct call and see what they look like in the direct call, to see if we can find a pattern.
 - Might want to discuss initialization with John
>> Extend ttsum to handle a case when it is not RAMCloud logs but instead independent TimeTrace
  - May want to add ttsum.py to the PerfUtils repository, since it is a useful combination
  - Sanity check: If we remove the locks, are we down to where we expect?
  - Does there exist a scheduler where we *never* need to acquire another core's lock?
    - That is, can we live partition well enough that we never need to actually
      rebalance?
    - Lock free queue for each worker (representing a core)?

 - At the moment, there isn't yet a need for them.
  - The userland scheduler will eventually need to be able to change priorities on threads running on other cores though, so you will need them.

 - We have written a dispatcher, not a scheduler yet.

--------------------------------------------------------------------------------
Thu 04 Feb 2016 01:45:01 AM PST


1. John's question: What does the expression     `mov    %fs:0xffffffffffffff98,%rbx` mean?
     http://stackoverflow.com/questions/10810203/what-is-the-fs-gs-register-intended-for 
    - It appears to be used for thread local storage
    - Once in yield to handle notRunning 
    - Once in run-thread to check running->stack.

2. Get rid of stackless threads at the Arachne bottom layer. DONE.
    - Two layers of abstraction
    - At the thread management layer, we're pushing the stackless threads concept up.
    - Assume threads have stacks and reject creation of you don't have a stack.
3. Experiment with data structures to try to preserve a fast context switch if possible.
    - Now the locks appear to be the bottleneck. Why are uncontended spinlocks
      so expensive?
      - Just to make absolute sure it is not cross-core cache traffic, let's
        switch back to using the main thread.
            - That is already switched.
4. Make the number of kernel threads dynamic and changeable.
5. Give meaningful names to different files inside Arachne. For example, the current thing is a dispatcher, so dispatcher.cc might be a better name than Arachne.cc.
6. Refactor Arachne's interaction with applications so that it defines main and applications replace their main with AppMain().

// Fix notes still
--------------------------------------------------------------------------------
Fri 05 Feb 2016 02:19:40 AM PST

0. Request for feedback on the talk.
0.2 Discuss meeting with Huawei and what we got out of that - event-ripping, dedicated queues. Why we can hope to do better in many ways.
0.3 - Ask about Rpc Levels again - can those be translated directly to priorities in the user threading scheme?

1. Discuss feedback from industry from the poster
     - FB thinks there are cycles that could be used, but wants to keep utilization low regardless
     - People are concerned about usability

     Kicking background jobs out - and killing them
      - OS tells the background cores are gone
      - Background jobs may be able to tell their distributed scheduler that
        they are about to lose cores.
    - Having the knowledge of resource assignments can help
      "Our app developers don't know anything about systems"
Only some need to know...
Wouldn't want every developer to know

Priorities

How we compute the levels and how we use the levels
 - Every RPC has a level
>> An Rpc at level n only invokes Rpcs less than n
>> Specify total concurrency level
    - As Rpcs come in, we process them in FIFO order until we hit our limit
    - At that point, we're going to go up more until we have 1 request for every level.
>> We only start a new request if level number is lower than any running request
 - Rpc levels can be done above level of Arachne

We may not want reads to starve writes
 - May want to give backup requests, and pings higher priority.
 - Other bookkeeping operations - index updates may want to have higher priorities.
 - Are we still @ risk of deadlock with 100 RPC's queued up
 - If any request uses a 1 MB stack, then eventually every stack may grow to that MB
 - Do the calculation based on the largest size a thread would actually need.
 - Buffers may be 2K
 - We could easily track stack usage  by using sentinel values.
 - Every Rpc has at least 1 or 2 buffers
 - If a server is issuing 60 
 - For cases where we need a big stack.
 - We need sentinels in the stack to check and crash if overflow.
 - 200k size stacks.
 - 5000 stacks
 - 1000 stacks is probably okay
Can we get rid of Rpc levels entirely?
 - Does priority need to be above Arachne?

Time to think about exactly what the API looks
 - API between Arachne and the higher level application software
    - RC WorkerManager
      - Does the higher level software create a bunch of threads  and let Arachne schedule them?
      - Or Just-in-time thread creation where arachne tells the higher level software it has spare core capacity?
 - Write out exactly what each of those APIs look like
 - Think about Arachne needing to negotiate with the kernel
 - Two layers of API - Arachne and OS, and Arachne and application

2. Removed the stackless threads
3. Context switch with locks and queues down to 28 ns from 150+ before, due to eliminating false sharing on locks.
    ==> optimism
    - Not sure if we have arrays of locks in RAMCloud that could benefit from this fix.

How often does my queue need to interact with other threads?
 - If those interactions are uncommon, use a separate mechanism for that.
 - Suppose I have a local queue and a special variable
 - Check the one variable - If I have other work, then the variable will change.
 - Variable will be in the same cache line as the WorkQueueLock
     - Indicate 
 - Separate fast path from the exception path.

Balance load without expensive locking.
Context switch blindingly fast.

For fast start/finish threads, always pay for global synchronizatoin
 - Otherwise for yielding threads we want to do fast context switch.

Stack magic is like pintos

--------------------------------------------------------------------------------
Wed 10 Feb 2016 03:37:21 PM PST

1. API Design
    - API between Arachne and higher level application software
        - API 1: higher layer software creates a bunch of threads with priorities and Arachne schedules them based on its knowledge of cores
        - API 2: WorkerManager does JIT thread creation when Arachne upcalls to tell it that it has spare cores.
    - API between Arachne and Kernel

2. Implementation to be faster than 28 ns for a context switch with a variable indicating whether we are busy or not.
    - In particular, the thread_yield mechanism only needs to read the variable
    and does not need to write it.
        - If any other thread needs to touch the queue,
   - I do not see how this is better than the SpinLock, which is also
     cache-resident if there is no contention from other cores.

3. Special mechanism for large stacks.

--------------------------------------------------------------------------------
Mailbox queue.

Dual-main approach
 - John says we should pick one and do one.
 - Doing two things.

Let's NOT take over main.


int initial_thread_func() {
// Create tasks, spawn other threads
}
int main() {
    Arachne::Arachne libraryInit(initial_thread_func);
}

int main() {
    Arachne::Arachne library(numCoresNeeded);
    // Create tasks, spawn other threads

    library.callMeAtEndOfMain(); // must be last call
}

Does Arachne start up more than one core at the start of execution?

How do I get acess to a global Arachne object everywhere?
 - Does it make sense to have an Arachne object?
 - Or just static method?


Great if the API looks like std::thread().


int main() {
//    Arachne::Arachne library(numCoresNeeded);
    // Arachne::createThread

    // This syntax feels simpler than asking the user to write a closure /
    // lambda function
    Arachne::thread firstArachneThread(func, arg1, arg2,...); // Also, the first created thread shouldaalso create the library. Cores can be allocated on demand.
    Arachne::thread secondArachneThread(func, arg1, arg2,...);
    ...
    Arachne::takeMyThread(); // must be last call
}

TODO: 
 - Look at std::thread
 - Assume for starters that we should do everything in that library
 - May need to look at implementation of the thread library.

Should we just them Arachne threads?
 "Task"

 One thread per request.
 Ambiguity between the thing we want to do, vs the thing that is doing the work.

 ArachneThread

TODO: Suppose I said the word task, what does that connote to you?
TODO: If I said a task is the same as a thread, would that feel natural to you?

Linux threads instead of kernel threads to distinguish.

--------------------------------------------------------------------------------
Wed 17 Feb 2016 12:49:41 AM PST

>> The people I asked in lab seemed to think task is not quite right, and are
>> in favor of fiber. I do not mind calling them fibers, but I'll stick with
>> ArachneThread for now.

Go over a better API.
Is there anything else needed in the user API to match with std::thread?
Kernel API.

0. Pinning other threads in Linux. Need to use a pthread call on the underlying handle, but haven't tested this yet.
    http://man7.org/linux/man-pages/man3/pthread_setaffinity_np.3.html
    http://stackoverflow.com/questions/13134186/c11-stdthreads-vs-posix-threads

1. Naming of fiber vs task. ArachneThreads.
2. New kernel API. Start by discussing the one that we had last week as a
   reminder so we can explain why we're doing the new one.
3. User API addition on top of std::thread and differences with std::thread. (ie, no detach).
    - John wants to be close to std::thread library to reduce the need for arguments with others for not using it.
    - Also if you return a thread pointer, how do you know whether the pointer is still valid?
        - Might as well have a thread ID.
    - Does thread::join return a value or not?
        - No, it returns void, so you can just be a no-opt when the thread has already exited.

3.5. gcc bug with trying to capturing arguments and lambdas, worked around with std::bind.

4. Read parts source code for std::thread library.
    - We should be able to use lock_guard directly.
    - Learn about perfect forwarding and std::forward and lvalue vs rvalue references, since that is how arguments are handled in std::thread
        /home/hq6/Code/src_for_oss_projects/gcc/libstdc++-v3/include/std/thread
        /home/hq6/Code/src_for_oss_projects/gcc/libstdc++-v3/src/c++11/thread.cc
        /home/hq6/Code/src_for_oss_projects/gcc/libgcc/gthr-posix.h 
        http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_detach.html
    _M_start_thread(_S_make_state(
              std::__bind_simple(std::forward<_Callable>(__f),
                             std::forward<_Args>(__args)...)),
                                    __depend);
 >>Implement the same semantic for Arachne function forwarding.
 - There is a thread object std::thread returned to the user (essentially a
   glorified handle), but it is decoupled from the underlying internal thread
   management and that is why we do not have a null pointer problem.

// Internal thread state should be allocated from a pool, whereas the external thread layer might just be a stack object. 
    The question is, be there any reason for the user thread state to be either non-moveable or non-copyable?
    - std::thread is moveable but not copyable.
    - Still need to think about this issue.
 - How does the external state check if the internal state is still active?
 - ie, to see if it has completed.
 - That's the reason for the detach, I suspect because otherwise the internal state object has to stay alive?

Move-constructable (rvalue reference from genuine rvalue or std::move) means a transfer of underlying resources so creation of one requires effective destruction  of another.
Copy-constructable (lvalue reference) implies a deep copy.
--------------------------------------------------------------------------------

Wed 24 Feb 2016 12:04:35 AM PST

1. Discuss the Google offer. Discuss Slicer and Slicer Memory. Limitations and ambitions. Architectural changes. Application Hints.
    - Tradeoffs of possibly expanding my thesis.
    - MSR systems group 
    // This has been discussed. I will defer the offer until I submit a paper for Arachne to  SOSP.

2. Measurement of thread creation overhead in Arachne today.
 - When we measure thread creation time, there are actually two separate overheads that we should measure.
      1. Thread creation overhead within the creating function. e.g. if we had // compare with std::thread
         the stack creation in the dispatch thread every few ms.
        - Let's do this first.
      2. Latency from createThread to thread actually running.
      3. Time to start and then join a thread (divide-by-n test)
        - This can be measured with create-join, and also with n-create n-join.

3. Discuss the API run-through
    - Thoughts on whether we could sustain this every 10 ms.


